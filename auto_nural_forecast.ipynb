{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2631334-f586-4474-aab0-1dd82e2abcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "k_df=pd.read_json(\"cleaned_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3214bea-6d62-4fff-bb06-38471fe2e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date                 locationName  value\n",
      "0     2020-01-01  1000 Hakaniemen kauppahalli   1.49\n",
      "1     2020-01-02  1000 Hakaniemen kauppahalli   7.47\n",
      "2     2020-01-03  1000 Hakaniemen kauppahalli   6.05\n",
      "3     2020-01-04  1000 Hakaniemen kauppahalli   6.53\n",
      "4     2020-01-05  1000 Hakaniemen kauppahalli  54.61\n",
      "...          ...                          ...    ...\n",
      "1761  2024-10-27  1000 Hakaniemen kauppahalli   0.71\n",
      "1762  2024-10-28  1000 Hakaniemen kauppahalli  22.59\n",
      "1763  2024-10-29  1000 Hakaniemen kauppahalli  22.46\n",
      "1764  2024-10-30  1000 Hakaniemen kauppahalli  21.69\n",
      "1765  2024-10-31  1000 Hakaniemen kauppahalli   0.02\n",
      "\n",
      "[1766 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the timestamp to datetime format\n",
    "k_df['timestamp'] = pd.to_datetime(k_df.index)\n",
    "\n",
    "# Now we will group by date and locationName, and sum the values.\n",
    "k_df['date'] = k_df['timestamp'].dt.date  # Extract date from timestamp\n",
    "\n",
    "daily_consumption = k_df.groupby(['date', 'locationName'], as_index=False)['value'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(daily_consumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5287410b-3d7d-4fda-ae27-f611c947a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_consumption['unique_id']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c15098-1a2b-4957-b3cb-f70b065b125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2020-01-01').date()  # Convert to datetime.date\n",
    "end_date = pd.to_datetime('2024-08-31').date()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54434941-0858-49e7-bb49-6f625915e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=daily_consumption[(daily_consumption['date'] >= start_date) & (daily_consumption['date'] <= end_date)]\n",
    "rest_df=daily_consumption[(daily_consumption['date']) > end_date].iloc[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63031430-18c1-49c3-969d-32d4a0deb01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7767dc-aeac-45c5-88b5-bb3e60d338e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571bc817-bb4f-4c06-9074-b3304473f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = pd.DataFrame({\n",
    "    'ds': df['date'],  # Timestamps as 'ds'\n",
    "    'y': df['value'],  # The actual data as 'y'\n",
    "    'unique_id':df['unique_id']  # The unique identifier\n",
    "\n",
    "})\n",
    "check_df= pd.DataFrame({\n",
    "    'ds': rest_df['date'],  # Timestamps as 'ds'\n",
    "    'actual': rest_df['value'],  # The actual data as 'y'\n",
    "    'unique_id':rest_df['unique_id'] } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5fdedc0-4f00-4f6b-ad4a-cc4fc3ff106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f8d013-96d1-414a-8a97-477555e429a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoNHITS, AutoLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bac15cf-f6d9-44e0-a17d-a145ef9a6888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=42188)\u001b[0m C:\\Users\\35841\\Desktop\\energy_consumption_modeling\\energyEV\\Lib\\site-packages\\ray\\tune\\integration\\pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m Seed set to 5\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m   | Name            | Type          | Params | Mode \n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 0 | loss            | MAE           | 0      | train\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 1 | padder          | ConstantPad1d | 0      | train\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 2 | scaler          | TemporalNorm  | 0      | train\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 3 | hist_encoder    | LSTM          | 122 K  | train\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 4 | context_adapter | Linear        | 303 K  | train\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 5 | mlp_decoder     | MLP           | 26.6 K | train\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 451 K     Trainable params\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 451 K     Total params\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 1.806     Total estimated model params size (MB)\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 11        Modules in train mode\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.330, train_loss_epoch=1.330]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.400, train_loss_epoch=1.400]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808]        \n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 10.49it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=0.808]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]       \n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s, v_num=0, train_loss_step=1.330, train_loss_epoch=1.330]\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.330, train_loss_epoch=1.330]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.796, train_loss_epoch=0.796]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.709, train_loss_epoch=0.709]        \n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00, 15.62it/s, v_num=0, train_loss_step=1.280, train_loss_epoch=1.280]\n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.715, train_loss_epoch=0.715]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.330, train_loss_epoch=1.330]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.682, train_loss_epoch=0.682]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.722, train_loss_epoch=0.722]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.460, train_loss_epoch=1.460]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140]        \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 15.11it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=1.140]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.300, train_loss_epoch=1.300]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.320, train_loss_epoch=1.320]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.835, train_loss_epoch=0.835]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.280, train_loss_epoch=1.280]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.987, train_loss_epoch=0.987]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.858, train_loss_epoch=0.858]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.450, train_loss_epoch=1.450]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.831, train_loss_epoch=0.831]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.787, train_loss_epoch=0.787]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.330, train_loss_epoch=1.330]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.805, train_loss_epoch=0.805]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.748, train_loss_epoch=0.748]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.360, train_loss_epoch=1.360]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.793, train_loss_epoch=0.793]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.777, train_loss_epoch=0.777]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.896, train_loss_epoch=0.896]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811]        \n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 12.93it/s, v_num=0, train_loss_step=1.240, train_loss_epoch=1.240]\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.240, train_loss_epoch=1.240]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.931, train_loss_epoch=0.931]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.990, train_loss_epoch=0.990]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.977, train_loss_epoch=0.977]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.676, train_loss_epoch=0.676]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.793, train_loss_epoch=0.793]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.982, train_loss_epoch=0.982]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 14.40it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=0.957]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210, valid_loss=10.20]        \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210, valid_loss=10.20]\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=10.20]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.20]        \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.981, train_loss_epoch=0.981, valid_loss=10.20]        \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=10.20]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810, valid_loss=10.20]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=10.20]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791, valid_loss=10.20]        \n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00, 14.18it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=10.20]\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=10.20]        \n",
      "Epoch 115: 100%|██████████| 1/1 [00:00<00:00, 13.35it/s, v_num=0, train_loss_step=0.974, train_loss_epoch=1.020, valid_loss=10.20]\n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.974, train_loss_epoch=0.974, valid_loss=10.20]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200, valid_loss=10.20]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210, valid_loss=10.20]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.967, train_loss_epoch=0.967, valid_loss=10.20]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843, valid_loss=10.20]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.20]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.973, train_loss_epoch=0.973, valid_loss=10.20]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811, valid_loss=10.20]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130, valid_loss=10.20]        \n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00, 15.62it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.20]\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.20]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=10.20]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.840, train_loss_epoch=0.840, valid_loss=10.20]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=10.20]        \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 13.27it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=10.20]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=10.20]        \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 13.68it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=10.20]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=10.20]        \n",
      "Epoch 139: 100%|██████████| 1/1 [00:00<00:00, 15.50it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=10.20]\n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=10.20]        \n",
      "Epoch 141: 100%|██████████| 1/1 [00:00<00:00, 13.42it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.20]\n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.20]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.250, valid_loss=10.20]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.370, train_loss_epoch=1.370, valid_loss=10.20]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.350, train_loss_epoch=1.350, valid_loss=10.20]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.20]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885, valid_loss=10.20]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.20]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=10.20]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.20]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=10.20]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.882, train_loss_epoch=0.882, valid_loss=10.20]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.707, train_loss_epoch=0.707, valid_loss=10.20]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=10.20]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.864, train_loss_epoch=0.864, valid_loss=10.20]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.250, valid_loss=10.20]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942, valid_loss=10.20]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.380, train_loss_epoch=1.380, valid_loss=10.20]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.682, train_loss_epoch=0.682, valid_loss=10.20]        \n",
      "Epoch 171: 100%|██████████| 1/1 [00:00<00:00, 14.10it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.996, valid_loss=10.20]\n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.996, valid_loss=10.20]        \n",
      "Epoch 173: 100%|██████████| 1/1 [00:00<00:00, 15.04it/s, v_num=0, train_loss_step=0.766, train_loss_epoch=0.766, valid_loss=10.20]\n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.766, train_loss_epoch=0.766, valid_loss=10.20]        \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=10.20]        \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.864, train_loss_epoch=0.864, valid_loss=10.20]        \n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 19.46it/s, v_num=0, train_loss_step=1.310, train_loss_epoch=1.170, valid_loss=10.20]\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.310, train_loss_epoch=1.310, valid_loss=10.20]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.20]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=10.20]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=10.20]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=10.20]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.874, train_loss_epoch=0.874, valid_loss=10.20]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=10.20]        \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909, valid_loss=10.20]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.240, train_loss_epoch=1.240, valid_loss=10.20]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=10.20]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.830, train_loss_epoch=0.830, valid_loss=10.20]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.20]        \n",
      "Epoch 198: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=10.20]\n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=10.20]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 18.96it/s, v_num=0, train_loss_step=1.280, train_loss_epoch=0.917, valid_loss=10.20]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.280, train_loss_epoch=1.280, valid_loss=10.10]        \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=10.10]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.10]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=10.10]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.10]        \n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 15.70it/s, v_num=0, train_loss_step=0.984, train_loss_epoch=1.030, valid_loss=10.10]\n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.984, train_loss_epoch=0.984, valid_loss=10.10]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.10]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.10]        \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.980, train_loss_epoch=0.980, valid_loss=10.10]        \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.752, train_loss_epoch=0.752, valid_loss=10.10]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.10]        \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=10.10]        \n",
      "Epoch 217: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811, valid_loss=10.10]\n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811, valid_loss=10.10]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.986, train_loss_epoch=0.986, valid_loss=10.10]        \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791, valid_loss=10.10]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.873, valid_loss=10.10]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.10]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955, valid_loss=10.10]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=10.10]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909, valid_loss=10.10]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808, valid_loss=10.10]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=10.10]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.829, train_loss_epoch=0.829, valid_loss=10.10]        \n",
      "Epoch 233: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s, v_num=0, train_loss_step=0.760, train_loss_epoch=0.760, valid_loss=10.10]\n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.760, train_loss_epoch=0.760, valid_loss=10.10]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=10.10]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=10.10]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.896, train_loss_epoch=0.896, valid_loss=10.10]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=10.10]        \n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=10.10]        \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.10]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=10.10]        \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.692, train_loss_epoch=0.692, valid_loss=10.10]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.794, train_loss_epoch=0.794, valid_loss=10.10]        \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899, valid_loss=10.10]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.250, valid_loss=10.10]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.842, train_loss_epoch=0.842, valid_loss=10.10]        \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=10.10]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899, valid_loss=10.10]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.688, train_loss_epoch=0.688, valid_loss=10.10]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=10.10]        \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=10.10]        \n",
      "Epoch 264: 100%|██████████| 1/1 [00:00<00:00, 15.05it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200, valid_loss=10.10]\n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200, valid_loss=10.10]        \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.10]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.972, train_loss_epoch=0.972, valid_loss=10.10]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.350, train_loss_epoch=1.350, valid_loss=10.10]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.320, train_loss_epoch=1.320, valid_loss=10.10]        \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.838, train_loss_epoch=0.838, valid_loss=10.10]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.831, train_loss_epoch=0.831, valid_loss=10.10]        \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=10.10]        \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.838, train_loss_epoch=0.838, valid_loss=10.10]        \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230, valid_loss=10.10]        \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.908, train_loss_epoch=0.908, valid_loss=10.10]        \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.10]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=10.10]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.728, train_loss_epoch=0.728, valid_loss=10.10]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.945, train_loss_epoch=0.945, valid_loss=10.10]        \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.10]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=10.10]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=10.10]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816, valid_loss=10.10]        \n",
      "Epoch 296: 100%|██████████| 1/1 [00:00<00:00, 13.81it/s, v_num=0, train_loss_step=0.835, train_loss_epoch=0.835, valid_loss=10.10]\n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.835, train_loss_epoch=0.835, valid_loss=10.10]        \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.10]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 18.33it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.180, valid_loss=10.10]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\u001b[A\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=8.920]        \n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=8.920]        \n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=8.920]        \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.920]        \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=8.920]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=8.920]        \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.995, train_loss_epoch=0.995, valid_loss=8.920]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.990, train_loss_epoch=0.990, valid_loss=8.920]        \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888, valid_loss=8.920]        \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=8.920]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.920]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.920]        \n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.920]        \n",
      "Epoch 322: 100%|██████████| 1/1 [00:00<00:00, 16.84it/s, v_num=0, train_loss_step=0.760, train_loss_epoch=0.760, valid_loss=8.920]\n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.760, train_loss_epoch=0.760, valid_loss=8.920]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847, valid_loss=8.920]        \n",
      "Epoch 326:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.920]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.804, train_loss_epoch=0.804, valid_loss=8.920]        \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.830, train_loss_epoch=0.830, valid_loss=8.920]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=8.920]        \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=8.920]        \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=8.920]        \n",
      "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=8.920]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.859, train_loss_epoch=0.859, valid_loss=8.920]        \n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=8.920]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=8.920]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.967, train_loss_epoch=0.967, valid_loss=8.920]        \n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=8.920]        \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130, valid_loss=8.920]        \n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=8.920]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210, valid_loss=8.920]        \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.320, train_loss_epoch=1.320, valid_loss=8.920]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.873, valid_loss=8.920]        \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=8.920]        \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=8.920]        \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=8.920]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.815, train_loss_epoch=0.815, valid_loss=8.920]        \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=8.920]        \n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=8.920]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.756, train_loss_epoch=0.756, valid_loss=8.920]        \n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867, valid_loss=8.920]        \n",
      "Epoch 363: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867, valid_loss=8.920]\n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.240, train_loss_epoch=1.240, valid_loss=8.920]        \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.250, valid_loss=8.920]        \n",
      "Epoch 365: 100%|██████████| 1/1 [00:00<00:00, 13.55it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=8.920]\n",
      "Epoch 366:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=8.920]        \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.340, train_loss_epoch=1.340, valid_loss=8.920]        \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=8.920]        \n",
      "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=8.920]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.866, train_loss_epoch=0.866, valid_loss=8.920]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.920]        \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.813, train_loss_epoch=0.813, valid_loss=8.920]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.796, train_loss_epoch=0.796, valid_loss=8.920]        \n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953, valid_loss=8.920]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=8.920]        \n",
      "Epoch 380: 100%|██████████| 1/1 [00:00<00:00, 13.91it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.920]\n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.920]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=8.920]        \n",
      "Epoch 384: 100%|██████████| 1/1 [00:00<00:00, 12.99it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=0.884, valid_loss=8.920]\n",
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=8.920]        \n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=8.920]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=8.920]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.920]        \n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.920]        \n",
      "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.920]        \n",
      "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=8.920]        \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811, valid_loss=8.920]        \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=8.920]        \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.790, train_loss_epoch=0.790, valid_loss=8.920]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929, valid_loss=8.920]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 17.75it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=0.900, valid_loss=8.920]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\u001b[A\n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200, valid_loss=11.80]        \n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.745, train_loss_epoch=0.745, valid_loss=11.80]        \n",
      "Epoch 401: 100%|██████████| 1/1 [00:00<00:00, 15.13it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.745, valid_loss=11.80]\n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=11.80]        \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.993, train_loss_epoch=0.993, valid_loss=11.80]        \n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.954, train_loss_epoch=0.954, valid_loss=11.80]        \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.891, valid_loss=11.80]        \n",
      "Epoch 409: 100%|██████████| 1/1 [00:00<00:00, 13.88it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=11.80]\n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.686, train_loss_epoch=0.686, valid_loss=11.80]        \n",
      "Epoch 412:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994, valid_loss=11.80]        \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839, valid_loss=11.80]        \n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=11.80]        \n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=11.80]        \n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=11.80]        \n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.799, train_loss_epoch=0.799, valid_loss=11.80]        \n",
      "Epoch 422: 100%|██████████| 1/1 [00:00<00:00, 16.55it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=11.80]\n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=11.80]        \n",
      "Epoch 425:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=11.80]        \n",
      "Epoch 426:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=11.80]        \n",
      "Epoch 427:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798, valid_loss=11.80]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.829, train_loss_epoch=0.829, valid_loss=11.80]        \n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=11.80]        \n",
      "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911, valid_loss=11.80]        \n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.713, train_loss_epoch=0.713, valid_loss=11.80]        \n",
      "Epoch 436:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839, valid_loss=11.80]        \n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=11.80]        \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.709, train_loss_epoch=0.709, valid_loss=11.80]        \n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.958, train_loss_epoch=0.958, valid_loss=11.80]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=11.80]        \n",
      "Epoch 445:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953, valid_loss=11.80]        \n",
      "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=11.80]        \n",
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=11.80]        \n",
      "Epoch 450: 100%|██████████| 1/1 [00:00<00:00, 12.99it/s, v_num=0, train_loss_step=0.706, train_loss_epoch=0.706, valid_loss=11.80]\n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.706, train_loss_epoch=0.706, valid_loss=11.80]        \n",
      "Epoch 453:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.674, train_loss_epoch=0.674, valid_loss=11.80]        \n",
      "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=11.80]        \n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.270, train_loss_epoch=1.270, valid_loss=11.80]        \n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929, valid_loss=11.80]        \n",
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=11.80]        \n",
      "Epoch 462:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.972, train_loss_epoch=0.972, valid_loss=11.80]        \n",
      "Epoch 464:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230, valid_loss=11.80]        \n",
      "Epoch 465:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=11.80]        \n",
      "Epoch 466:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.718, train_loss_epoch=0.718, valid_loss=11.80]        \n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.861, train_loss_epoch=0.861, valid_loss=11.80]        \n",
      "Epoch 469:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905, valid_loss=11.80]        \n",
      "Epoch 469: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905, valid_loss=11.80]\n",
      "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=11.80]        \n",
      "Epoch 471:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=11.80]        \n",
      "Epoch 472:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=11.80]        \n",
      "Epoch 473:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=11.80]        \n",
      "Epoch 474:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.240, train_loss_epoch=1.240, valid_loss=11.80]        \n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.814, train_loss_epoch=0.814, valid_loss=11.80]        \n",
      "Epoch 478:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.687, train_loss_epoch=0.687, valid_loss=11.80]        \n",
      "Epoch 479: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=1.160, valid_loss=11.80]\n",
      "Epoch 480:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=11.80]        \n",
      "Epoch 481: 100%|██████████| 1/1 [00:00<00:00, 15.60it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=11.80]\n",
      "Epoch 482:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=11.80]        \n",
      "Epoch 484:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=11.80]        \n",
      "Epoch 485:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=11.80]        \n",
      "Epoch 487:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.859, train_loss_epoch=0.859, valid_loss=11.80]        \n",
      "Epoch 489:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=11.80]        \n",
      "Epoch 491:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=11.80]        \n",
      "Epoch 493:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927, valid_loss=11.80]        \n",
      "Epoch 495:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=11.80]        \n",
      "Epoch 497:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.898, train_loss_epoch=0.898, valid_loss=11.80]        \n",
      "Epoch 499:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=11.80]        \n",
      "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 15.99it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.872, valid_loss=11.80]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\u001b[A\n",
      "Epoch 500:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=10.60]        \n",
      "Epoch 502:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=10.60]        \n",
      "Epoch 504:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.825, train_loss_epoch=0.825, valid_loss=10.60]        \n",
      "Epoch 506:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950, valid_loss=10.60]        \n",
      "Epoch 507:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.60]        \n",
      "Epoch 507: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791, valid_loss=10.60]\n",
      "Epoch 508:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791, valid_loss=10.60]        \n",
      "Epoch 510:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.775, train_loss_epoch=0.775, valid_loss=10.60]        \n",
      "Epoch 512:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=10.60]        \n",
      "Epoch 513: 100%|██████████| 1/1 [00:00<00:00, 13.39it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=10.60]\n",
      "Epoch 514:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=10.60]        \n",
      "Epoch 516:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881, valid_loss=10.60]        \n",
      "Epoch 517: 100%|██████████| 1/1 [00:00<00:00, 13.77it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=10.60]\n",
      "Epoch 518:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=10.60]        \n",
      "Epoch 520:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.785, train_loss_epoch=0.785, valid_loss=10.60]        \n",
      "Epoch 522:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.792, train_loss_epoch=0.792, valid_loss=10.60]        \n",
      "Epoch 523:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.858, train_loss_epoch=0.858, valid_loss=10.60]        \n",
      "Epoch 524:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.891, valid_loss=10.60]        \n",
      "Epoch 525:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.833, train_loss_epoch=0.833, valid_loss=10.60]        \n",
      "Epoch 526:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.310, train_loss_epoch=1.310, valid_loss=10.60]        \n",
      "Epoch 527:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.931, train_loss_epoch=0.931, valid_loss=10.60]        \n",
      "Epoch 529:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839, valid_loss=10.60]        \n",
      "Epoch 531:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911, valid_loss=10.60]        \n",
      "Epoch 533:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.60]        \n",
      "Epoch 535:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.820, train_loss_epoch=0.820, valid_loss=10.60]        \n",
      "Epoch 537:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130, valid_loss=10.60]        \n",
      "Epoch 539:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.60]        \n",
      "Epoch 541:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.871, train_loss_epoch=0.871, valid_loss=10.60]        \n",
      "Epoch 542:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=10.60]        \n",
      "Epoch 544:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=10.60]        \n",
      "Epoch 546:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=10.60]        \n",
      "Epoch 548:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.873, valid_loss=10.60]        \n",
      "Epoch 549: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.60]\n",
      "Epoch 550:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.60]        \n",
      "Epoch 551:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=10.60]        \n",
      "Epoch 552:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.60]        \n",
      "Epoch 553:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=10.60]        \n",
      "Epoch 554:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803, valid_loss=10.60]        \n",
      "Epoch 555:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850, valid_loss=10.60]        \n",
      "Epoch 557:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=10.60]        \n",
      "Epoch 559:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.60]        \n",
      "Epoch 561:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.844, train_loss_epoch=0.844, valid_loss=10.60]        \n",
      "Epoch 562:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=10.60]        \n",
      "Epoch 562: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.928, valid_loss=10.60]\n",
      "Epoch 563:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=10.60]        \n",
      "Epoch 564:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=10.60]        \n",
      "Epoch 566:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.60]        \n",
      "Epoch 568:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=10.60]        \n",
      "Epoch 570:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.842, train_loss_epoch=0.842, valid_loss=10.60]        \n",
      "Epoch 572:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885, valid_loss=10.60]        \n",
      "Epoch 574:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.688, train_loss_epoch=0.688, valid_loss=10.60]        \n",
      "Epoch 576:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.971, train_loss_epoch=0.971, valid_loss=10.60]        \n",
      "Epoch 577:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.933, train_loss_epoch=0.933, valid_loss=10.60]        \n",
      "Epoch 578:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=10.60]        \n",
      "Epoch 579:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.60]        \n",
      "Epoch 580:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=10.60]        \n",
      "Epoch 581:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.931, train_loss_epoch=0.931, valid_loss=10.60]        \n",
      "Epoch 583:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940, valid_loss=10.60]        \n",
      "Epoch 585:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.876, train_loss_epoch=0.876, valid_loss=10.60]        \n",
      "Epoch 587:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940, valid_loss=10.60]        \n",
      "Epoch 589:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809, valid_loss=10.60]        \n",
      "Epoch 591:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.60]        \n",
      "Epoch 593:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.60]        \n",
      "Epoch 594:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.976, train_loss_epoch=0.976, valid_loss=10.60]        \n",
      "Epoch 596:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=10.60]        \n",
      "Epoch 598:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=10.60]        \n",
      "Epoch 599: 100%|██████████| 1/1 [00:00<00:00, 16.00it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=1.180, valid_loss=10.60]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\u001b[A\n",
      "Epoch 599: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=1.180, valid_loss=8.400]\n",
      "Epoch 600:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839, valid_loss=8.400]        \n",
      "Epoch 601:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=8.400]        \n",
      "Epoch 602:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=8.400]        \n",
      "Epoch 603: 100%|██████████| 1/1 [00:00<00:00, 15.26it/s, v_num=0, train_loss_step=0.694, train_loss_epoch=0.694, valid_loss=8.400]\n",
      "Epoch 604:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.694, train_loss_epoch=0.694, valid_loss=8.400]        \n",
      "Epoch 605:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.400]        \n",
      "Epoch 607:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=8.400]        \n",
      "Epoch 609:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=8.400]        \n",
      "Epoch 611:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=8.400]        \n",
      "Epoch 613:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.762, train_loss_epoch=0.762, valid_loss=8.400]        \n",
      "Epoch 614:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810, valid_loss=8.400]        \n",
      "Epoch 615:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=8.400]        \n",
      "Epoch 616:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=8.400]        \n",
      "Epoch 617:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=8.400]        \n",
      "Epoch 619:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=8.400]        \n",
      "Epoch 620:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=8.400]        \n",
      "Epoch 622:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=8.400]        \n",
      "Epoch 624:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.990, train_loss_epoch=0.990, valid_loss=8.400]        \n",
      "Epoch 626:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.951, train_loss_epoch=0.951, valid_loss=8.400]        \n",
      "Epoch 628:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=8.400]        \n",
      "Epoch 629:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.975, train_loss_epoch=0.975, valid_loss=8.400]        \n",
      "Epoch 630:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.976, train_loss_epoch=0.976, valid_loss=8.400]        \n",
      "Epoch 631:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.817, train_loss_epoch=0.817, valid_loss=8.400]        \n",
      "Epoch 633:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=8.400]        \n",
      "Epoch 635:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=8.400]        \n",
      "Epoch 637:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.828, train_loss_epoch=0.828, valid_loss=8.400]        \n",
      "Epoch 639:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=8.400]        \n",
      "Epoch 641:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.826, train_loss_epoch=0.826, valid_loss=8.400]        \n",
      "Epoch 643:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=8.400]        \n",
      "Epoch 644:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.987, train_loss_epoch=0.987, valid_loss=8.400]        \n",
      "Epoch 645:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.794, train_loss_epoch=0.794, valid_loss=8.400]        \n",
      "Epoch 646:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939, valid_loss=8.400]        \n",
      "Epoch 648:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=8.400]        \n",
      "Epoch 650:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.984, train_loss_epoch=0.984, valid_loss=8.400]        \n",
      "Epoch 652:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.400]        \n",
      "Epoch 654:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=8.400]        \n",
      "Epoch 655:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941, valid_loss=8.400]        \n",
      "Epoch 657:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.848, train_loss_epoch=0.848, valid_loss=8.400]        \n",
      "Epoch 659:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=8.400]        \n",
      "Epoch 661:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=8.400]        \n",
      "Epoch 662: 100%|██████████| 1/1 [00:00<00:00, 16.54it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=8.400]\n",
      "Epoch 663:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=8.400]        \n",
      "Epoch 664:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808, valid_loss=8.400]        \n",
      "Epoch 665:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.908, train_loss_epoch=0.908, valid_loss=8.400]        \n",
      "Epoch 666:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=8.400]        \n",
      "Epoch 668:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=8.400]        \n",
      "Epoch 670:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=8.400]        \n",
      "Epoch 672:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.778, train_loss_epoch=0.778, valid_loss=8.400]        \n",
      "Epoch 674:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.860, train_loss_epoch=0.860, valid_loss=8.400]        \n",
      "Epoch 675:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.871, train_loss_epoch=0.871, valid_loss=8.400]        \n",
      "Epoch 675: 100%|██████████| 1/1 [00:00<00:00, 14.87it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=8.400]\n",
      "Epoch 676:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=8.400]        \n",
      "Epoch 677:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=8.400]        \n",
      "Epoch 679:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=8.400]        \n",
      "Epoch 681:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.838, train_loss_epoch=0.838, valid_loss=8.400]        \n",
      "Epoch 683:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.400]        \n",
      "Epoch 685:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.848, train_loss_epoch=0.848, valid_loss=8.400]        \n",
      "Epoch 686:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=8.400]        \n",
      "Epoch 687:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955, valid_loss=8.400]        \n",
      "Epoch 688:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.794, train_loss_epoch=0.794, valid_loss=8.400]        \n",
      "Epoch 690:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=8.400]        \n",
      "Epoch 692:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.852, train_loss_epoch=0.852, valid_loss=8.400]        \n",
      "Epoch 694:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.915, train_loss_epoch=0.915, valid_loss=8.400]        \n",
      "Epoch 696:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=8.400]        \n",
      "Epoch 698:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.400]        \n",
      "Epoch 699: 100%|██████████| 1/1 [00:00<00:00, 19.72it/s, v_num=0, train_loss_step=0.815, train_loss_epoch=0.915, valid_loss=8.400]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Epoch 700:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.815, train_loss_epoch=0.815, valid_loss=8.370]        \n",
      "Epoch 701:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.370]        \n",
      "Epoch 703:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997, valid_loss=8.370]        \n",
      "Epoch 705:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.370]        \n",
      "Epoch 707:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.370]        \n",
      "Epoch 708:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=8.370]        \n",
      "Epoch 710:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=8.370]        \n",
      "Epoch 712:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.802, train_loss_epoch=0.802, valid_loss=8.370]        \n",
      "Epoch 714:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=8.370]        \n",
      "Epoch 716:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=8.370]        \n",
      "Epoch 718:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.840, train_loss_epoch=0.840, valid_loss=8.370]        \n",
      "Epoch 719:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857, valid_loss=8.370]        \n",
      "Epoch 720:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.370]        \n",
      "Epoch 721:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=8.370]        \n",
      "Epoch 723:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881, valid_loss=8.370]        \n",
      "Epoch 724:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=8.370]        \n",
      "Epoch 725:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=8.370]        \n",
      "Epoch 727:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=8.370]        \n",
      "Epoch 729:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.784, train_loss_epoch=0.784, valid_loss=8.370]        \n",
      "Epoch 730:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.956, train_loss_epoch=0.956, valid_loss=8.370]        \n",
      "Epoch 732:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.370]        \n",
      "Epoch 734:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962, valid_loss=8.370]        \n",
      "Epoch 736:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=8.370]        \n",
      "Epoch 737: 100%|██████████| 1/1 [00:00<00:00, 15.52it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=8.370]\n",
      "Epoch 738:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=8.370]        \n",
      "Epoch 739: 100%|██████████| 1/1 [00:00<00:00, 16.05it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960, valid_loss=8.370]\n",
      "Epoch 740:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960, valid_loss=8.370]        \n",
      "Epoch 741:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.865, train_loss_epoch=0.865, valid_loss=8.370]        \n",
      "Epoch 742:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.738, train_loss_epoch=0.738, valid_loss=8.370]        \n",
      "Epoch 743:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.370]        \n",
      "Epoch 744:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=8.370]        \n",
      "Epoch 746:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.270, train_loss_epoch=1.270, valid_loss=8.370]        \n",
      "Epoch 747:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816, valid_loss=8.370]        \n",
      "Epoch 748:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=8.370]        \n",
      "Epoch 749: 100%|██████████| 1/1 [00:00<00:00, 19.64it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=0.798, valid_loss=8.370]\n",
      "Epoch 750:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210, valid_loss=8.370]        \n",
      "Epoch 752:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=8.370]        \n",
      "Epoch 753:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=8.370]        \n",
      "Epoch 753: 100%|██████████| 1/1 [00:00<00:00, 19.05it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=8.370]\n",
      "Epoch 754:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=8.370]        \n",
      "Epoch 755:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.818, train_loss_epoch=0.818, valid_loss=8.370]        \n",
      "Epoch 756:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=8.370]        \n",
      "Epoch 757:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.370]        \n",
      "Epoch 758:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=8.370]        \n",
      "Epoch 760:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.709, train_loss_epoch=0.709, valid_loss=8.370]        \n",
      "Epoch 761:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.981, train_loss_epoch=0.981, valid_loss=8.370]        \n",
      "Epoch 763:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=8.370]        \n",
      "Epoch 765:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=8.370]        \n",
      "Epoch 766:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=8.370]        \n",
      "Epoch 768:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=8.370]        \n",
      "Epoch 770:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.963, train_loss_epoch=0.963, valid_loss=8.370]        \n",
      "Epoch 772:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.971, train_loss_epoch=0.971, valid_loss=8.370]        \n",
      "Epoch 773:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.825, train_loss_epoch=0.825, valid_loss=8.370]        \n",
      "Epoch 774:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.370]        \n",
      "Epoch 775:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=8.370]        \n",
      "Epoch 777:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130, valid_loss=8.370]        \n",
      "Epoch 779:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=8.370]        \n",
      "Epoch 781:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.370]        \n",
      "Epoch 782:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857, valid_loss=8.370]        \n",
      "Epoch 784:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798, valid_loss=8.370]        \n",
      "Epoch 785:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.793, train_loss_epoch=0.793, valid_loss=8.370]        \n",
      "Epoch 786:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=8.370]        \n",
      "Epoch 788:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810, valid_loss=8.370]        \n",
      "Epoch 790:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.992, train_loss_epoch=0.992, valid_loss=8.370]        \n",
      "Epoch 792:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.818, train_loss_epoch=0.818, valid_loss=8.370]        \n",
      "Epoch 793:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=8.370]        \n",
      "Epoch 795:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.370]        \n",
      "Epoch 797:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=8.370]        \n",
      "Epoch 799:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=8.370]        \n",
      "Epoch 799: 100%|██████████| 1/1 [00:00<00:00, 13.95it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.849, valid_loss=8.370]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\u001b[A\n",
      "Epoch 800:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.080]        \n",
      "Epoch 802:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.695, train_loss_epoch=0.695, valid_loss=8.080]        \n",
      "Epoch 804:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816, valid_loss=8.080]        \n",
      "Epoch 806:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994, valid_loss=8.080]        \n",
      "Epoch 808:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=8.080]        \n",
      "Epoch 809:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=8.080]        \n",
      "Epoch 811:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.891, valid_loss=8.080]        \n",
      "Epoch 813:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.842, train_loss_epoch=0.842, valid_loss=8.080]        \n",
      "Epoch 815:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=8.080]        \n",
      "Epoch 817:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997, valid_loss=8.080]        \n",
      "Epoch 819:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=8.080]        \n",
      "Epoch 820:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.765, train_loss_epoch=0.765, valid_loss=8.080]        \n",
      "Epoch 822:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962, valid_loss=8.080]        \n",
      "Epoch 824:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=8.080]        \n",
      "Epoch 826:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.915, train_loss_epoch=0.915, valid_loss=8.080]        \n",
      "Epoch 827:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=8.080]        \n",
      "Epoch 828:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.918, train_loss_epoch=0.918, valid_loss=8.080]        \n",
      "Epoch 829:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.771, train_loss_epoch=0.771, valid_loss=8.080]        \n",
      "Epoch 830:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.896, train_loss_epoch=0.896, valid_loss=8.080]        \n",
      "Epoch 831:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=8.080]        \n",
      "Epoch 833:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.789, train_loss_epoch=0.789, valid_loss=8.080]        \n",
      "Epoch 835:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.660, train_loss_epoch=0.660, valid_loss=8.080]        \n",
      "Epoch 837:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=8.080]        \n",
      "Epoch 839:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=8.080]        \n",
      "Epoch 841:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.828, train_loss_epoch=0.828, valid_loss=8.080]        \n",
      "Epoch 843:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.832, train_loss_epoch=0.832, valid_loss=8.080]        \n",
      "Epoch 844:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.749, train_loss_epoch=0.749, valid_loss=8.080]        \n",
      "Epoch 845:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809, valid_loss=8.080]        \n",
      "Epoch 846:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.896, train_loss_epoch=0.896, valid_loss=8.080]        \n",
      "Epoch 848:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=8.080]        \n",
      "Epoch 850:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=8.080]        \n",
      "Epoch 852:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.874, train_loss_epoch=0.874, valid_loss=8.080]        \n",
      "Epoch 853:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=8.080]        \n",
      "Epoch 854:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.978, train_loss_epoch=0.978, valid_loss=8.080]        \n",
      "Epoch 855:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=8.080]        \n",
      "Epoch 856:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.832, train_loss_epoch=0.832, valid_loss=8.080]        \n",
      "Epoch 858:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=8.080]        \n",
      "Epoch 859:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.080]        \n",
      "Epoch 861:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=8.080]        \n",
      "Epoch 863:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.080]        \n",
      "Epoch 865:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=8.080]        \n",
      "Epoch 866: 100%|██████████| 1/1 [00:00<00:00, 17.14it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=8.080]\n",
      "Epoch 867:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.766, train_loss_epoch=0.766, valid_loss=8.080]        \n",
      "Epoch 868:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.815, train_loss_epoch=0.815, valid_loss=8.080]        \n",
      "Epoch 869:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.080]        \n",
      "Epoch 870:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.741, train_loss_epoch=0.741, valid_loss=8.080]        \n",
      "Epoch 872:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.748, train_loss_epoch=0.748, valid_loss=8.080]        \n",
      "Epoch 874:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.897, train_loss_epoch=0.897, valid_loss=8.080]        \n",
      "Epoch 876:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942, valid_loss=8.080]        \n",
      "Epoch 878:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.937, train_loss_epoch=0.937, valid_loss=8.080]        \n",
      "Epoch 880:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.773, train_loss_epoch=0.773, valid_loss=8.080]        \n",
      "Epoch 881:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=8.080]        \n",
      "Epoch 883:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=8.080]        \n",
      "Epoch 885:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=8.080]        \n",
      "Epoch 887:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.866, train_loss_epoch=0.866, valid_loss=8.080]        \n",
      "Epoch 889:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.797, train_loss_epoch=0.797, valid_loss=8.080]        \n",
      "Epoch 891:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=8.080]        \n",
      "Epoch 892:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.792, train_loss_epoch=0.792, valid_loss=8.080]        \n",
      "Epoch 892: 100%|██████████| 1/1 [00:00<00:00, 18.51it/s, v_num=0, train_loss_step=0.792, train_loss_epoch=0.792, valid_loss=8.080]\n",
      "Epoch 893:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=8.080]        \n",
      "Epoch 895:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.963, train_loss_epoch=0.963, valid_loss=8.080]        \n",
      "Epoch 896:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=8.080]        \n",
      "Epoch 898:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.762, train_loss_epoch=0.762, valid_loss=8.080]        \n",
      "Epoch 899: 100%|██████████| 1/1 [00:00<00:00, 14.71it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.992, valid_loss=8.080]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\u001b[A\n",
      "Epoch 899: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.992, valid_loss=10.10]\n",
      "Epoch 900:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=10.10]        \n",
      "Epoch 901:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.10]        \n",
      "Epoch 903:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=10.10]        \n",
      "Epoch 905:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.10]        \n",
      "Epoch 907:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.790, train_loss_epoch=0.790, valid_loss=10.10]        \n",
      "Epoch 909:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851, valid_loss=10.10]        \n",
      "Epoch 910:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847, valid_loss=10.10]        \n",
      "Epoch 911:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847, valid_loss=10.10]\n",
      "Epoch 912: 100%|██████████| 1/1 [00:00<00:00, 20.02it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.740, valid_loss=10.10]\n",
      "Epoch 913:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.740, valid_loss=10.10]        \n",
      "Epoch 914: 100%|██████████| 1/1 [00:00<00:00, 16.50it/s, v_num=0, train_loss_step=0.823, train_loss_epoch=0.823, valid_loss=10.10]\n",
      "Epoch 915:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.823, train_loss_epoch=0.823, valid_loss=10.10]        \n",
      "Epoch 916:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808, valid_loss=10.10]        \n",
      "Epoch 918:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.804, train_loss_epoch=0.804, valid_loss=10.10]        \n",
      "Epoch 920:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=10.10]        \n",
      "Epoch 922:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.10]        \n",
      "Epoch 924:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=10.10]        \n",
      "Epoch 925:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=10.10]        \n",
      "Epoch 926:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=10.10]        \n",
      "Epoch 927:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=10.10]        \n",
      "Epoch 929:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=10.10]        \n",
      "Epoch 931:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=10.10]        \n",
      "Epoch 933:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950, valid_loss=10.10]        \n",
      "Epoch 934:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.799, train_loss_epoch=0.799, valid_loss=10.10]        \n",
      "Epoch 936:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.818, train_loss_epoch=0.818, valid_loss=10.10]        \n",
      "Epoch 938:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=10.10]        \n",
      "Epoch 939:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.715, train_loss_epoch=0.715, valid_loss=10.10]        \n",
      "Epoch 940:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.761, train_loss_epoch=0.761, valid_loss=10.10]        \n",
      "Epoch 941:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.813, train_loss_epoch=0.813, valid_loss=10.10]        \n",
      "Epoch 943:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=10.10]        \n",
      "Epoch 945:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791, valid_loss=10.10]        \n",
      "Epoch 946:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=10.10]        \n",
      "Epoch 947:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.802, train_loss_epoch=0.802, valid_loss=10.10]        \n",
      "Epoch 949:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.945, train_loss_epoch=0.945, valid_loss=10.10]        \n",
      "Epoch 950:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.771, train_loss_epoch=0.771, valid_loss=10.10]        \n",
      "Epoch 952:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960, valid_loss=10.10]        \n",
      "Epoch 954:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.819, train_loss_epoch=0.819, valid_loss=10.10]        \n",
      "Epoch 956:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803, valid_loss=10.10]        \n",
      "Epoch 957:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.10]        \n",
      "Epoch 957: 100%|██████████| 1/1 [00:00<00:00, 17.99it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803, valid_loss=10.10]\n",
      "Epoch 958:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803, valid_loss=10.10]        \n",
      "Epoch 960:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.805, train_loss_epoch=0.805, valid_loss=10.10]        \n",
      "Epoch 961:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.10]        \n",
      "Epoch 962:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.802, train_loss_epoch=0.802, valid_loss=10.10]        \n",
      "Epoch 963: 100%|██████████| 1/1 [00:00<00:00, 16.13it/s, v_num=0, train_loss_step=0.932, train_loss_epoch=0.932, valid_loss=10.10]\n",
      "Epoch 964:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.932, train_loss_epoch=0.932, valid_loss=10.10]        \n",
      "Epoch 965:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=10.10]        \n",
      "Epoch 967:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.685, train_loss_epoch=0.685, valid_loss=10.10]        \n",
      "Epoch 969:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.790, train_loss_epoch=0.790, valid_loss=10.10]        \n",
      "Epoch 971:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=10.10]        \n",
      "Epoch 972:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.727, train_loss_epoch=0.727, valid_loss=10.10]        \n",
      "Epoch 974:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.882, train_loss_epoch=0.882, valid_loss=10.10]        \n",
      "Epoch 976:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.782, train_loss_epoch=0.782, valid_loss=10.10]        \n",
      "Epoch 977:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.10]        \n",
      "Epoch 977: 100%|██████████| 1/1 [00:00<00:00, 14.79it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=10.10]\n",
      "Epoch 978:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=10.10]        \n",
      "Epoch 979:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.959, train_loss_epoch=0.959, valid_loss=10.10]        \n",
      "Epoch 980:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.815, train_loss_epoch=0.815, valid_loss=10.10]        \n",
      "Epoch 981:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=10.10]        \n",
      "Epoch 981: 100%|██████████| 1/1 [00:00<00:00, 14.15it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=10.10]\n",
      "Epoch 982:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=10.10]        \n",
      "Epoch 984:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.804, train_loss_epoch=0.804, valid_loss=10.10]        \n",
      "Epoch 985:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.778, train_loss_epoch=0.778, valid_loss=10.10]        \n",
      "Epoch 987:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.10]        \n",
      "Epoch 989:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.729, train_loss_epoch=0.729, valid_loss=10.10]        \n",
      "Epoch 990: 100%|██████████| 1/1 [00:00<00:00, 13.90it/s, v_num=0, train_loss_step=0.684, train_loss_epoch=0.684, valid_loss=10.10]\n",
      "Epoch 991:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.684, train_loss_epoch=0.684, valid_loss=10.10]        \n",
      "Epoch 992:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=10.10]        \n",
      "Epoch 993:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=10.10]        \n",
      "Epoch 994:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.982, train_loss_epoch=0.982, valid_loss=10.10]        \n",
      "Epoch 995:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.10]        \n",
      "Epoch 996:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=10.10]        \n",
      "Epoch 997:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=10.10]        \n",
      "Epoch 998:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.693, train_loss_epoch=0.693, valid_loss=10.10]        \n",
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s, v_num=0, train_loss_step=0.746, train_loss_epoch=0.904, valid_loss=10.10]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=42188)\u001b[0m `Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=42188)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\u001b[A\n",
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s, v_num=0, train_loss_step=0.746, train_loss_epoch=0.746, valid_loss=7.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=3332)\u001b[0m C:\\Users\\35841\\Desktop\\energy_consumption_modeling\\energyEV\\Lib\\site-packages\\ray\\tune\\integration\\pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m Seed set to 2\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m   | Name            | Type          | Params | Mode \n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 0 | loss            | MAE           | 0      | train\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 1 | padder          | ConstantPad1d | 0      | train\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 2 | scaler          | TemporalNorm  | 0      | train\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 3 | hist_encoder    | LSTM          | 1.1 M  | train\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 4 | context_adapter | Linear        | 90.3 K | train\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 5 | mlp_decoder     | MLP           | 3.6 K  | train\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 1.2 M     Trainable params\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 1.2 M     Total params\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 4.720     Total estimated model params size (MB)\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 11        Modules in train mode\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.998, train_loss_epoch=0.998]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.996]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.991, train_loss_epoch=0.991]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.991, train_loss_epoch=0.991]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.991, train_loss_epoch=0.991]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.987, train_loss_epoch=0.987]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.980, train_loss_epoch=0.980]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.974, train_loss_epoch=0.974]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.966, train_loss_epoch=0.966]        \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.963, train_loss_epoch=0.963]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961]        \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.954, train_loss_epoch=0.954]        \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.944, train_loss_epoch=0.944]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.944, train_loss_epoch=0.944]        \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.944, train_loss_epoch=0.944]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.944, train_loss_epoch=0.944]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.937, train_loss_epoch=0.937]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.933, train_loss_epoch=0.933]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.918, train_loss_epoch=0.918]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.948, train_loss_epoch=0.948]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922]        \n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924]\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.914]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.944, train_loss_epoch=0.944]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.932, train_loss_epoch=0.932]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.925, train_loss_epoch=0.925]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.901, train_loss_epoch=0.901]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.900, train_loss_epoch=0.900]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.911]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881, valid_loss=8.180]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888, valid_loss=8.180]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885, valid_loss=8.180]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.876, train_loss_epoch=0.876, valid_loss=8.180]        \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=8.180]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=8.180]        \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.180]        \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.876, train_loss_epoch=0.876, valid_loss=8.180]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=8.180]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=8.180]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867, valid_loss=8.180]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=8.180]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.874, train_loss_epoch=0.874, valid_loss=8.180]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867, valid_loss=8.180]        \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.864, train_loss_epoch=0.864, valid_loss=8.180]        \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=8.180]        \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.859, train_loss_epoch=0.859, valid_loss=8.180]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.853, train_loss_epoch=0.853, valid_loss=8.180]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.853, train_loss_epoch=0.853, valid_loss=8.180]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.896, train_loss_epoch=0.896, valid_loss=8.180]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=8.180]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=8.180]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950, valid_loss=8.180]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927, valid_loss=8.180]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927, valid_loss=8.180]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.908, train_loss_epoch=0.908, valid_loss=8.180]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=8.180]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888, valid_loss=8.180]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=8.180]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=8.180]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867, valid_loss=8.180]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983, valid_loss=8.180]        \n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983, valid_loss=8.180]\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.976, train_loss_epoch=0.976, valid_loss=8.180]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939, valid_loss=8.180]        \n",
      "Epoch 133: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, v_num=0, train_loss_step=0.945, train_loss_epoch=0.945, valid_loss=8.180]\n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.945, train_loss_epoch=0.945, valid_loss=8.180]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=8.180]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.966, train_loss_epoch=0.966, valid_loss=8.180]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=8.180]        \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.961, valid_loss=8.180]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950, valid_loss=8.180]        \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.938, train_loss_epoch=0.938, valid_loss=8.180]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926, valid_loss=8.180]        \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.926, valid_loss=8.180]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=8.180]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=8.180]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=8.180]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909, valid_loss=8.180]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.901, train_loss_epoch=0.901, valid_loss=8.180]        \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=8.180]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.890, train_loss_epoch=0.890, valid_loss=8.180]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=8.180]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.880, train_loss_epoch=0.880, valid_loss=8.180]        \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=8.180]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.876, train_loss_epoch=0.876, valid_loss=8.180]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=8.180]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=8.180]        \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.865, train_loss_epoch=0.865, valid_loss=8.180]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.861, train_loss_epoch=0.861, valid_loss=8.180]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.860, train_loss_epoch=0.860, valid_loss=8.180]        \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s, v_num=0, train_loss_step=0.858, train_loss_epoch=0.858, valid_loss=8.180]\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.858, train_loss_epoch=0.858, valid_loss=8.180]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.852, train_loss_epoch=0.852, valid_loss=8.180]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.852, train_loss_epoch=0.852, valid_loss=8.180]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850, valid_loss=8.180]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.180]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.844, train_loss_epoch=0.844, valid_loss=8.180]        \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.838, train_loss_epoch=0.838, valid_loss=8.180]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837, valid_loss=8.180]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.832, train_loss_epoch=0.832, valid_loss=8.180]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.831, train_loss_epoch=0.831, valid_loss=8.180]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.826, train_loss_epoch=0.826, valid_loss=8.180]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=8.180]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.818, train_loss_epoch=0.818, valid_loss=8.180]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=8.180]        \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=8.180]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.807, train_loss_epoch=0.807, valid_loss=8.180]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843, valid_loss=8.180]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811, valid_loss=8.180]        \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.871, train_loss_epoch=0.871, valid_loss=8.180]        \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=8.180]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.180]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.876, train_loss_epoch=0.876, valid_loss=8.180]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857, valid_loss=8.180]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=8.180]        \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.860, train_loss_epoch=0.860, valid_loss=8.180]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857, valid_loss=8.180]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.855, train_loss_epoch=0.855, valid_loss=8.180]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.854, train_loss_epoch=0.854, valid_loss=8.180]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.859, train_loss_epoch=0.859, valid_loss=8.180]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=8.180]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=8.180]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.925, train_loss_epoch=0.925, valid_loss=8.180]        \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=8.180]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=8.180]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.923, train_loss_epoch=0.923, valid_loss=8.180]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=8.180]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=8.180]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=8.180]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=8.180]        \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=8.180]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.900, train_loss_epoch=0.900, valid_loss=8.180]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.896, train_loss_epoch=0.896, valid_loss=8.180]        \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=8.180]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.892, valid_loss=8.180]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888, valid_loss=8.790]        \n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=8.790]        \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.875, train_loss_epoch=0.875, valid_loss=8.790]        \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.880, train_loss_epoch=0.880, valid_loss=8.790]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.790]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=8.790]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.886, train_loss_epoch=0.886, valid_loss=8.790]        \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.882, train_loss_epoch=0.882, valid_loss=8.790]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.880, train_loss_epoch=0.880, valid_loss=8.790]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.875, train_loss_epoch=0.875, valid_loss=8.790]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.871, train_loss_epoch=0.871, valid_loss=8.790]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.869, train_loss_epoch=0.869, valid_loss=8.790]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.869, train_loss_epoch=0.869, valid_loss=8.790]        \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.859, train_loss_epoch=0.859, valid_loss=8.790]        \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.865, train_loss_epoch=0.865, valid_loss=8.790]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.880, train_loss_epoch=0.880, valid_loss=8.790]        \n",
      "Epoch 215: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.790]\n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.790]        \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.868, train_loss_epoch=0.868, valid_loss=8.790]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=8.790]        \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=8.790]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=8.790]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.846, train_loss_epoch=0.846, valid_loss=8.790]        \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.790]        \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.846, train_loss_epoch=0.846, valid_loss=8.790]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850, valid_loss=8.790]        \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.852, train_loss_epoch=0.852, valid_loss=8.790]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.790]        \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837, valid_loss=8.790]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.835, train_loss_epoch=0.835, valid_loss=8.790]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=8.790]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.823, train_loss_epoch=0.823, valid_loss=8.790]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.815, train_loss_epoch=0.815, valid_loss=8.790]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=8.790]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=8.790]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816, valid_loss=8.790]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=8.790]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.818, train_loss_epoch=0.818, valid_loss=8.790]        \n",
      "Epoch 236: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.790]\n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.790]        \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.790]\n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=8.790]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.859, train_loss_epoch=0.859, valid_loss=8.790]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=8.790]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=8.790]        \n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.890, train_loss_epoch=0.890, valid_loss=8.790]        \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.891, valid_loss=8.790]        \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887, valid_loss=8.790]        \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.875, train_loss_epoch=0.875, valid_loss=8.790]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.861, train_loss_epoch=0.861, valid_loss=8.790]        \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=8.790]        \n",
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=8.790]\n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=8.790]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=8.790]        \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857, valid_loss=8.790]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851, valid_loss=8.790]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.842, train_loss_epoch=0.842, valid_loss=8.790]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837, valid_loss=8.790]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.836, train_loss_epoch=0.836, valid_loss=8.790]        \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.833, train_loss_epoch=0.833, valid_loss=8.790]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.829, train_loss_epoch=0.829, valid_loss=8.790]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.828, train_loss_epoch=0.828, valid_loss=8.790]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=8.790]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.814, train_loss_epoch=0.814, valid_loss=8.790]        \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=8.790]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.801, train_loss_epoch=0.801, valid_loss=8.790]        \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.799, train_loss_epoch=0.799, valid_loss=8.790]        \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.790, train_loss_epoch=0.790, valid_loss=8.790]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.790, train_loss_epoch=0.790, valid_loss=8.790]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.786, valid_loss=8.790]        \n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.787, train_loss_epoch=0.787, valid_loss=8.790]        \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.777, train_loss_epoch=0.777, valid_loss=8.790]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.776, train_loss_epoch=0.776, valid_loss=8.790]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.771, train_loss_epoch=0.771, valid_loss=8.790]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.768, train_loss_epoch=0.768, valid_loss=8.790]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.761, train_loss_epoch=0.761, valid_loss=8.790]        \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.762, train_loss_epoch=0.762, valid_loss=8.790]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=8.790]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.754, train_loss_epoch=0.754, valid_loss=8.790]        \n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=8.790]        \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=8.790]        \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.753, train_loss_epoch=0.753, valid_loss=8.790]        \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=8.790]        \n",
      "Epoch 278: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, v_num=0, train_loss_step=0.741, train_loss_epoch=0.741, valid_loss=8.790]\n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.741, train_loss_epoch=0.741, valid_loss=8.790]        \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.745, train_loss_epoch=0.745, valid_loss=8.790]        \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.746, train_loss_epoch=0.746, valid_loss=8.790]        \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.742, train_loss_epoch=0.742, valid_loss=8.790]        \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.777, train_loss_epoch=0.777, valid_loss=8.790]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.745, train_loss_epoch=0.745, valid_loss=8.790]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.750, train_loss_epoch=0.750, valid_loss=8.790]        \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=8.790]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=8.790]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.731, train_loss_epoch=0.731, valid_loss=8.790]        \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=8.790]        \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.747, train_loss_epoch=0.747, valid_loss=8.790]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.738, train_loss_epoch=0.738, valid_loss=8.790]        \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.771, train_loss_epoch=0.771, valid_loss=8.790]        \n",
      "Epoch 292: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=8.790]\n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=8.790]        \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.766, train_loss_epoch=0.766, valid_loss=8.790]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.735, train_loss_epoch=0.735, valid_loss=8.790]        \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=8.790]        \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=8.790]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.731, train_loss_epoch=0.731, valid_loss=8.790]        \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.738, train_loss_epoch=0.738, valid_loss=8.790]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s, v_num=0, train_loss_step=0.721, train_loss_epoch=0.738, valid_loss=8.790]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.721, train_loss_epoch=0.721, valid_loss=7.990]        \n",
      "Epoch 300: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, v_num=0, train_loss_step=0.715, train_loss_epoch=0.715, valid_loss=7.990]\n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.715, train_loss_epoch=0.715, valid_loss=7.990]        \n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.723, train_loss_epoch=0.723, valid_loss=7.990]        \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.719, train_loss_epoch=0.719, valid_loss=7.990]        \n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.712, train_loss_epoch=0.712, valid_loss=7.990]        \n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.712, train_loss_epoch=0.712, valid_loss=7.990]        \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.707, train_loss_epoch=0.707, valid_loss=7.990]        \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.714, train_loss_epoch=0.714, valid_loss=7.990]        \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.705, train_loss_epoch=0.705, valid_loss=7.990]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.702, train_loss_epoch=0.702, valid_loss=7.990]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.701, train_loss_epoch=0.701, valid_loss=7.990]        \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.708, train_loss_epoch=0.708, valid_loss=7.990]        \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.697, train_loss_epoch=0.697, valid_loss=7.990]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.692, train_loss_epoch=0.692, valid_loss=7.990]        \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.691, train_loss_epoch=0.691, valid_loss=7.990]        \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.691, train_loss_epoch=0.691, valid_loss=7.990]        \n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.687, train_loss_epoch=0.687, valid_loss=7.990]        \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.688, train_loss_epoch=0.688, valid_loss=7.990]        \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=7.990]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.680, train_loss_epoch=0.680, valid_loss=7.990]        \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.680, train_loss_epoch=0.680, valid_loss=7.990]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.677, train_loss_epoch=0.677, valid_loss=7.990]        \n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.676, train_loss_epoch=0.676, valid_loss=7.990]        \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.672, train_loss_epoch=0.672, valid_loss=7.990]        \n",
      "Epoch 323: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s, v_num=0, train_loss_step=0.672, train_loss_epoch=0.672, valid_loss=7.990]\n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.671, train_loss_epoch=0.671, valid_loss=7.990]        \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.669, train_loss_epoch=0.669, valid_loss=7.990]        \n",
      "Epoch 326:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.667, train_loss_epoch=0.667, valid_loss=7.990]        \n",
      "Epoch 326: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s, v_num=0, train_loss_step=0.665, train_loss_epoch=0.665, valid_loss=7.990]\n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.665, train_loss_epoch=0.665, valid_loss=7.990]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.663, train_loss_epoch=0.663, valid_loss=7.990]        \n",
      "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.661, train_loss_epoch=0.661, valid_loss=7.990]        \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.659, train_loss_epoch=0.659, valid_loss=7.990]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.655, train_loss_epoch=0.655, valid_loss=7.990]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.653, train_loss_epoch=0.653, valid_loss=7.990]        \n",
      "Epoch 332: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s, v_num=0, train_loss_step=0.651, train_loss_epoch=0.651, valid_loss=7.990]\n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.651, train_loss_epoch=0.651, valid_loss=7.990]        \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.655, train_loss_epoch=0.655, valid_loss=7.990]        \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.732, train_loss_epoch=0.732, valid_loss=7.990]        \n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.743, train_loss_epoch=0.743, valid_loss=7.990]        \n",
      "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.696, train_loss_epoch=0.696, valid_loss=7.990]        \n",
      "Epoch 338:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=7.990]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.692, train_loss_epoch=0.692, valid_loss=7.990]        \n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.706, train_loss_epoch=0.706, valid_loss=7.990]        \n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.713, train_loss_epoch=0.713, valid_loss=7.990]        \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.701, train_loss_epoch=0.701, valid_loss=7.990]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.740, valid_loss=7.990]        \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.689, train_loss_epoch=0.689, valid_loss=7.990]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.684, train_loss_epoch=0.684, valid_loss=7.990]        \n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.697, train_loss_epoch=0.697, valid_loss=7.990]        \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=7.990]        \n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.682, train_loss_epoch=0.682, valid_loss=7.990]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.687, train_loss_epoch=0.687, valid_loss=7.990]        \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.680, train_loss_epoch=0.680, valid_loss=7.990]        \n",
      "Epoch 350: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, v_num=0, train_loss_step=0.669, train_loss_epoch=0.669, valid_loss=7.990]\n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.669, train_loss_epoch=0.669, valid_loss=7.990]        \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.662, train_loss_epoch=0.662, valid_loss=7.990]        \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.661, train_loss_epoch=0.661, valid_loss=7.990]        \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.663, train_loss_epoch=0.663, valid_loss=7.990]        \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.659, train_loss_epoch=0.659, valid_loss=7.990]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.653, train_loss_epoch=0.653, valid_loss=7.990]        \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.648, train_loss_epoch=0.648, valid_loss=7.990]        \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.649, train_loss_epoch=0.649, valid_loss=7.990]        \n",
      "Epoch 359:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.646, train_loss_epoch=0.646, valid_loss=7.990]        \n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.643, train_loss_epoch=0.643, valid_loss=7.990]        \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.639, train_loss_epoch=0.639, valid_loss=7.990]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.637, train_loss_epoch=0.637, valid_loss=7.990]        \n",
      "Epoch 362: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s, v_num=0, train_loss_step=0.637, train_loss_epoch=0.637, valid_loss=7.990]\n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.637, train_loss_epoch=0.637, valid_loss=7.990]        \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.634, train_loss_epoch=0.634, valid_loss=7.990]        \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.632, train_loss_epoch=0.632, valid_loss=7.990]        \n",
      "Epoch 366:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.628, train_loss_epoch=0.628, valid_loss=7.990]        \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.627, train_loss_epoch=0.627, valid_loss=7.990]        \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.625, train_loss_epoch=0.625, valid_loss=7.990]        \n",
      "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.623, train_loss_epoch=0.623, valid_loss=7.990]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.621, train_loss_epoch=0.621, valid_loss=7.990]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.619, train_loss_epoch=0.619, valid_loss=7.990]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.617, train_loss_epoch=0.617, valid_loss=7.990]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.616, train_loss_epoch=0.616, valid_loss=7.990]        \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.613, train_loss_epoch=0.613, valid_loss=7.990]        \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.612, train_loss_epoch=0.612, valid_loss=7.990]        \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.610, train_loss_epoch=0.610, valid_loss=7.990]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.609, train_loss_epoch=0.609, valid_loss=7.990]        \n",
      "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.607, train_loss_epoch=0.607, valid_loss=7.990]        \n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.605, train_loss_epoch=0.605, valid_loss=7.990]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.604, train_loss_epoch=0.604, valid_loss=7.990]        \n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.602, train_loss_epoch=0.602, valid_loss=7.990]        \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.600, train_loss_epoch=0.600, valid_loss=7.990]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.599, train_loss_epoch=0.599, valid_loss=7.990]        \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.598, train_loss_epoch=0.598, valid_loss=7.990]        \n",
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.596, train_loss_epoch=0.596, valid_loss=7.990]        \n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.595, train_loss_epoch=0.595, valid_loss=7.990]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.595, train_loss_epoch=0.595, valid_loss=7.990]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.593, train_loss_epoch=0.593, valid_loss=7.990]        \n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.591, train_loss_epoch=0.591, valid_loss=7.990]        \n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.590, train_loss_epoch=0.590, valid_loss=7.990]        \n",
      "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.588, train_loss_epoch=0.588, valid_loss=7.990]        \n",
      "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.587, train_loss_epoch=0.587, valid_loss=7.990]        \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.586, train_loss_epoch=0.586, valid_loss=7.990]        \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.584, train_loss_epoch=0.584, valid_loss=7.990]        \n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.583, train_loss_epoch=0.583, valid_loss=7.990]        \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.581, train_loss_epoch=0.581, valid_loss=7.990]        \n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.581, train_loss_epoch=0.581, valid_loss=7.990]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.581, train_loss_epoch=0.581, valid_loss=7.990]        \n",
      "Epoch 399:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.580, train_loss_epoch=0.580, valid_loss=7.990]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, v_num=0, train_loss_step=0.577, train_loss_epoch=0.580, valid_loss=7.990]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\u001b[A\n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.577, train_loss_epoch=0.577, valid_loss=6.510]        \n",
      "Epoch 400: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, v_num=0, train_loss_step=0.576, train_loss_epoch=0.576, valid_loss=6.510]\n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.576, train_loss_epoch=0.576, valid_loss=6.510]        \n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.576, train_loss_epoch=0.576, valid_loss=6.510]        \n",
      "Epoch 403:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.578, train_loss_epoch=0.578, valid_loss=6.510]        \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.586, train_loss_epoch=0.586, valid_loss=6.510]        \n",
      "Epoch 405:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.573, train_loss_epoch=0.573, valid_loss=6.510]        \n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.576, train_loss_epoch=0.576, valid_loss=6.510]        \n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.583, train_loss_epoch=0.583, valid_loss=6.510]        \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.568, train_loss_epoch=0.568, valid_loss=6.510]        \n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.581, train_loss_epoch=0.581, valid_loss=6.510]        \n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.569, train_loss_epoch=0.569, valid_loss=6.510]        \n",
      "Epoch 411:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.574, train_loss_epoch=0.574, valid_loss=6.510]        \n",
      "Epoch 412:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.565, train_loss_epoch=0.565, valid_loss=6.510]        \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.570, train_loss_epoch=0.570, valid_loss=6.510]        \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.564, train_loss_epoch=0.564, valid_loss=6.510]        \n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.565, train_loss_epoch=0.565, valid_loss=6.510]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.565, train_loss_epoch=0.565, valid_loss=6.510]        \n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.563, train_loss_epoch=0.563, valid_loss=6.510]        \n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.582, train_loss_epoch=0.582, valid_loss=6.510]        \n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.556, train_loss_epoch=0.556, valid_loss=6.510]        \n",
      "Epoch 420:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.578, train_loss_epoch=0.578, valid_loss=6.510]        \n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.581, train_loss_epoch=0.581, valid_loss=6.510]        \n",
      "Epoch 422:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.581, train_loss_epoch=0.581, valid_loss=6.510]        \n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.574, train_loss_epoch=0.574, valid_loss=6.510]        \n",
      "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.562, train_loss_epoch=0.562, valid_loss=6.510]        \n",
      "Epoch 425:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.574, train_loss_epoch=0.574, valid_loss=6.510]        \n",
      "Epoch 426:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.559, train_loss_epoch=0.559, valid_loss=6.510]        \n",
      "Epoch 426: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, v_num=0, train_loss_step=0.562, train_loss_epoch=0.562, valid_loss=6.510]\n",
      "Epoch 427:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.562, train_loss_epoch=0.562, valid_loss=6.510]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.562, train_loss_epoch=0.562, valid_loss=6.510]        \n",
      "Epoch 429:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.554, train_loss_epoch=0.554, valid_loss=6.510]        \n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.554, train_loss_epoch=0.554, valid_loss=6.510]        \n",
      "Epoch 431:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.551, train_loss_epoch=0.551, valid_loss=6.510]        \n",
      "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.548, train_loss_epoch=0.548, valid_loss=6.510]        \n",
      "Epoch 433:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.548, train_loss_epoch=0.548, valid_loss=6.510]        \n",
      "Epoch 433: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s, v_num=0, train_loss_step=0.543, train_loss_epoch=0.548, valid_loss=6.510]\n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.543, train_loss_epoch=0.543, valid_loss=6.510]        \n",
      "Epoch 435:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.541, train_loss_epoch=0.541, valid_loss=6.510]        \n",
      "Epoch 436:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.541, train_loss_epoch=0.541, valid_loss=6.510]        \n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.537, train_loss_epoch=0.537, valid_loss=6.510]        \n",
      "Epoch 438:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.536, train_loss_epoch=0.536, valid_loss=6.510]        \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.535, train_loss_epoch=0.535, valid_loss=6.510]        \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.532, train_loss_epoch=0.532, valid_loss=6.510]        \n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.531, train_loss_epoch=0.531, valid_loss=6.510]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.529, train_loss_epoch=0.529, valid_loss=6.510]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.527, train_loss_epoch=0.527, valid_loss=6.510]        \n",
      "Epoch 444:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.526, train_loss_epoch=0.526, valid_loss=6.510]        \n",
      "Epoch 445:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.525, train_loss_epoch=0.525, valid_loss=6.510]        \n",
      "Epoch 446:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.522, train_loss_epoch=0.522, valid_loss=6.510]        \n",
      "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.521, train_loss_epoch=0.521, valid_loss=6.510]        \n",
      "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.521, train_loss_epoch=0.521, valid_loss=6.510]        \n",
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.519, train_loss_epoch=0.519, valid_loss=6.510]        \n",
      "Epoch 450:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.518, train_loss_epoch=0.518, valid_loss=6.510]        \n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.517, train_loss_epoch=0.517, valid_loss=6.510]        \n",
      "Epoch 452:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513, valid_loss=6.510]        \n",
      "Epoch 453:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513, valid_loss=6.510]        \n",
      "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.512, train_loss_epoch=0.512, valid_loss=6.510]        \n",
      "Epoch 455:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.511, train_loss_epoch=0.511, valid_loss=6.510]        \n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513, valid_loss=6.510]        \n",
      "Epoch 457:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513, valid_loss=6.510]        \n",
      "Epoch 457: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513, valid_loss=6.510]\n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513, valid_loss=6.510]        \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.517, train_loss_epoch=0.517, valid_loss=6.510]        \n",
      "Epoch 459: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.517, valid_loss=6.510]\n",
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513, valid_loss=6.510]        \n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.527, train_loss_epoch=0.527, valid_loss=6.510]        \n",
      "Epoch 462:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.546, train_loss_epoch=0.546, valid_loss=6.510]        \n",
      "Epoch 463:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.688, train_loss_epoch=0.688, valid_loss=6.510]        \n",
      "Epoch 464:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.666, train_loss_epoch=0.666, valid_loss=6.510]        \n",
      "Epoch 465:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.657, train_loss_epoch=0.657, valid_loss=6.510]        \n",
      "Epoch 466:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.664, train_loss_epoch=0.664, valid_loss=6.510]        \n",
      "Epoch 467:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=6.510]        \n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.687, train_loss_epoch=0.687, valid_loss=6.510]        \n",
      "Epoch 469:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.680, train_loss_epoch=0.680, valid_loss=6.510]        \n",
      "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.658, train_loss_epoch=0.658, valid_loss=6.510]        \n",
      "Epoch 471:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.659, train_loss_epoch=0.659, valid_loss=6.510]        \n",
      "Epoch 472:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.653, train_loss_epoch=0.653, valid_loss=6.510]        \n",
      "Epoch 473:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.644, train_loss_epoch=0.644, valid_loss=6.510]        \n",
      "Epoch 474:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.645, train_loss_epoch=0.645, valid_loss=6.510]        \n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.632, train_loss_epoch=0.632, valid_loss=6.510]        \n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.628, train_loss_epoch=0.628, valid_loss=6.510]        \n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.623, train_loss_epoch=0.623, valid_loss=6.510]        \n",
      "Epoch 478:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.613, train_loss_epoch=0.613, valid_loss=6.510]        \n",
      "Epoch 479:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.614, train_loss_epoch=0.614, valid_loss=6.510]        \n",
      "Epoch 480:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.608, train_loss_epoch=0.608, valid_loss=6.510]        \n",
      "Epoch 481:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.603, train_loss_epoch=0.603, valid_loss=6.510]        \n",
      "Epoch 482:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.600, train_loss_epoch=0.600, valid_loss=6.510]        \n",
      "Epoch 483:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.594, train_loss_epoch=0.594, valid_loss=6.510]        \n",
      "Epoch 484:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.591, train_loss_epoch=0.591, valid_loss=6.510]        \n",
      "Epoch 485:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.584, train_loss_epoch=0.584, valid_loss=6.510]        \n",
      "Epoch 486:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.581, train_loss_epoch=0.581, valid_loss=6.510]        \n",
      "Epoch 487:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.583, train_loss_epoch=0.583, valid_loss=6.510]        \n",
      "Epoch 488:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.580, train_loss_epoch=0.580, valid_loss=6.510]        \n",
      "Epoch 489:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.582, train_loss_epoch=0.582, valid_loss=6.510]        \n",
      "Epoch 490:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.573, train_loss_epoch=0.573, valid_loss=6.510]        \n",
      "Epoch 491:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.571, train_loss_epoch=0.571, valid_loss=6.510]        \n",
      "Epoch 492:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.565, train_loss_epoch=0.565, valid_loss=6.510]        \n",
      "Epoch 493:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.563, train_loss_epoch=0.563, valid_loss=6.510]        \n",
      "Epoch 494:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.559, train_loss_epoch=0.559, valid_loss=6.510]        \n",
      "Epoch 495:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.553, train_loss_epoch=0.553, valid_loss=6.510]        \n",
      "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.551, train_loss_epoch=0.551, valid_loss=6.510]        \n",
      "Epoch 497:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.548, train_loss_epoch=0.548, valid_loss=6.510]        \n",
      "Epoch 498:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.543, train_loss_epoch=0.543, valid_loss=6.510]        \n",
      "Epoch 499:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.543, train_loss_epoch=0.543, valid_loss=6.510]        \n",
      "Epoch 499: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, v_num=0, train_loss_step=0.538, train_loss_epoch=0.543, valid_loss=6.510]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\u001b[A\n",
      "Epoch 500:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.538, train_loss_epoch=0.538, valid_loss=7.630]        \n",
      "Epoch 501:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.536, train_loss_epoch=0.536, valid_loss=7.630]        \n",
      "Epoch 502:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.533, train_loss_epoch=0.533, valid_loss=7.630]        \n",
      "Epoch 503:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.529, train_loss_epoch=0.529, valid_loss=7.630]        \n",
      "Epoch 504:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.528, train_loss_epoch=0.528, valid_loss=7.630]        \n",
      "Epoch 505:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.523, train_loss_epoch=0.523, valid_loss=7.630]        \n",
      "Epoch 506:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.522, train_loss_epoch=0.522, valid_loss=7.630]        \n",
      "Epoch 507:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.519, train_loss_epoch=0.519, valid_loss=7.630]        \n",
      "Epoch 508:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.516, train_loss_epoch=0.516, valid_loss=7.630]        \n",
      "Epoch 509:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.514, train_loss_epoch=0.514, valid_loss=7.630]        \n",
      "Epoch 510:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.512, train_loss_epoch=0.512, valid_loss=7.630]        \n",
      "Epoch 511:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.510, train_loss_epoch=0.510, valid_loss=7.630]        \n",
      "Epoch 512:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.508, train_loss_epoch=0.508, valid_loss=7.630]        \n",
      "Epoch 513:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.506, train_loss_epoch=0.506, valid_loss=7.630]        \n",
      "Epoch 514:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.504, train_loss_epoch=0.504, valid_loss=7.630]        \n",
      "Epoch 515:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.501, train_loss_epoch=0.501, valid_loss=7.630]        \n",
      "Epoch 516:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.500, train_loss_epoch=0.500, valid_loss=7.630]        \n",
      "Epoch 517:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.497, train_loss_epoch=0.497, valid_loss=7.630]        \n",
      "Epoch 518:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.495, train_loss_epoch=0.495, valid_loss=7.630]        \n",
      "Epoch 519:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.493, train_loss_epoch=0.493, valid_loss=7.630]        \n",
      "Epoch 520:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.491, train_loss_epoch=0.491, valid_loss=7.630]        \n",
      "Epoch 521:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.489, train_loss_epoch=0.489, valid_loss=7.630]        \n",
      "Epoch 522:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.487, train_loss_epoch=0.487, valid_loss=7.630]        \n",
      "Epoch 523:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.486, train_loss_epoch=0.486, valid_loss=7.630]        \n",
      "Epoch 524:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.484, train_loss_epoch=0.484, valid_loss=7.630]        \n",
      "Epoch 525:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.482, train_loss_epoch=0.482, valid_loss=7.630]        \n",
      "Epoch 526:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.481, train_loss_epoch=0.481, valid_loss=7.630]        \n",
      "Epoch 527:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.480, train_loss_epoch=0.480, valid_loss=7.630]        \n",
      "Epoch 528:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.479, train_loss_epoch=0.479, valid_loss=7.630]        \n",
      "Epoch 529:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.477, train_loss_epoch=0.477, valid_loss=7.630]        \n",
      "Epoch 530:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.477, train_loss_epoch=0.477, valid_loss=7.630]        \n",
      "Epoch 531:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.476, train_loss_epoch=0.476, valid_loss=7.630]        \n",
      "Epoch 532:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.474, train_loss_epoch=0.474, valid_loss=7.630]        \n",
      "Epoch 533:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.475, train_loss_epoch=0.475, valid_loss=7.630]        \n",
      "Epoch 534:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.473, train_loss_epoch=0.473, valid_loss=7.630]        \n",
      "Epoch 535:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.472, train_loss_epoch=0.472, valid_loss=7.630]        \n",
      "Epoch 536:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.472, train_loss_epoch=0.472, valid_loss=7.630]        \n",
      "Epoch 537:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.467, train_loss_epoch=0.467, valid_loss=7.630]        \n",
      "Epoch 538:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.466, train_loss_epoch=0.466, valid_loss=7.630]        \n",
      "Epoch 539:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.464, train_loss_epoch=0.464, valid_loss=7.630]        \n",
      "Epoch 540:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.465, train_loss_epoch=0.465, valid_loss=7.630]        \n",
      "Epoch 541:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.466, train_loss_epoch=0.466, valid_loss=7.630]        \n",
      "Epoch 542:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.469, train_loss_epoch=0.469, valid_loss=7.630]        \n",
      "Epoch 543:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.465, train_loss_epoch=0.465, valid_loss=7.630]        \n",
      "Epoch 544:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.462, train_loss_epoch=0.462, valid_loss=7.630]        \n",
      "Epoch 545:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.460, train_loss_epoch=0.460, valid_loss=7.630]        \n",
      "Epoch 546:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.459, train_loss_epoch=0.459, valid_loss=7.630]        \n",
      "Epoch 547:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.464, train_loss_epoch=0.464, valid_loss=7.630]        \n",
      "Epoch 548:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.459, train_loss_epoch=0.459, valid_loss=7.630]        \n",
      "Epoch 549:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.455, train_loss_epoch=0.455, valid_loss=7.630]        \n",
      "Epoch 550:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.454, train_loss_epoch=0.454, valid_loss=7.630]        \n",
      "Epoch 551:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.453, train_loss_epoch=0.453, valid_loss=7.630]        \n",
      "Epoch 552:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.452, train_loss_epoch=0.452, valid_loss=7.630]        \n",
      "Epoch 553:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.452, train_loss_epoch=0.452, valid_loss=7.630]        \n",
      "Epoch 554:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.450, train_loss_epoch=0.450, valid_loss=7.630]        \n",
      "Epoch 555:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.449, train_loss_epoch=0.449, valid_loss=7.630]        \n",
      "Epoch 556:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.446, train_loss_epoch=0.446, valid_loss=7.630]        \n",
      "Epoch 557:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.444, train_loss_epoch=0.444, valid_loss=7.630]        \n",
      "Epoch 558:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.444, train_loss_epoch=0.444, valid_loss=7.630]        \n",
      "Epoch 559:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.443, train_loss_epoch=0.443, valid_loss=7.630]        \n",
      "Epoch 560:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.444, train_loss_epoch=0.444, valid_loss=7.630]        \n",
      "Epoch 561:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.445, train_loss_epoch=0.445, valid_loss=7.630]        \n",
      "Epoch 562:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.443, train_loss_epoch=0.443, valid_loss=7.630]        \n",
      "Epoch 563:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.439, train_loss_epoch=0.439, valid_loss=7.630]        \n",
      "Epoch 564:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.439, train_loss_epoch=0.439, valid_loss=7.630]        \n",
      "Epoch 565:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.458, train_loss_epoch=0.458, valid_loss=7.630]        \n",
      "Epoch 566:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.447, train_loss_epoch=0.447, valid_loss=7.630]        \n",
      "Epoch 567:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.454, train_loss_epoch=0.454, valid_loss=7.630]        \n",
      "Epoch 568:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.448, train_loss_epoch=0.448, valid_loss=7.630]        \n",
      "Epoch 569:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.447, train_loss_epoch=0.447, valid_loss=7.630]        \n",
      "Epoch 570:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.446, train_loss_epoch=0.446, valid_loss=7.630]        \n",
      "Epoch 571:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.444, train_loss_epoch=0.444, valid_loss=7.630]        \n",
      "Epoch 572:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.445, train_loss_epoch=0.445, valid_loss=7.630]        \n",
      "Epoch 573:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.439, train_loss_epoch=0.439, valid_loss=7.630]        \n",
      "Epoch 574:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.441, train_loss_epoch=0.441, valid_loss=7.630]        \n",
      "Epoch 575:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.440, train_loss_epoch=0.440, valid_loss=7.630]        \n",
      "Epoch 576:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.435, train_loss_epoch=0.435, valid_loss=7.630]        \n",
      "Epoch 577:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.437, train_loss_epoch=0.437, valid_loss=7.630]        \n",
      "Epoch 578:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.433, train_loss_epoch=0.433, valid_loss=7.630]        \n",
      "Epoch 579:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.430, train_loss_epoch=0.430, valid_loss=7.630]        \n",
      "Epoch 580:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.430, train_loss_epoch=0.430, valid_loss=7.630]        \n",
      "Epoch 581:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.430, train_loss_epoch=0.430, valid_loss=7.630]        \n",
      "Epoch 582:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.427, train_loss_epoch=0.427, valid_loss=7.630]        \n",
      "Epoch 583:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.426, train_loss_epoch=0.426, valid_loss=7.630]        \n",
      "Epoch 584:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.425, train_loss_epoch=0.425, valid_loss=7.630]        \n",
      "Epoch 585:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.424, train_loss_epoch=0.424, valid_loss=7.630]        \n",
      "Epoch 586:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421, valid_loss=7.630]        \n",
      "Epoch 587:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421, valid_loss=7.630]        \n",
      "Epoch 588:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421, valid_loss=7.630]        \n",
      "Epoch 589:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.419, train_loss_epoch=0.419, valid_loss=7.630]        \n",
      "Epoch 590:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.419, train_loss_epoch=0.419, valid_loss=7.630]        \n",
      "Epoch 591:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.417, train_loss_epoch=0.417, valid_loss=7.630]        \n",
      "Epoch 592:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.417, train_loss_epoch=0.417, valid_loss=7.630]        \n",
      "Epoch 593:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.415, train_loss_epoch=0.415, valid_loss=7.630]        \n",
      "Epoch 594:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.413, train_loss_epoch=0.413, valid_loss=7.630]        \n",
      "Epoch 595:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.410, train_loss_epoch=0.410, valid_loss=7.630]        \n",
      "Epoch 596:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.412, train_loss_epoch=0.412, valid_loss=7.630]        \n",
      "Epoch 597:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.408, train_loss_epoch=0.408, valid_loss=7.630]        \n",
      "Epoch 598:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.410, train_loss_epoch=0.410, valid_loss=7.630]        \n",
      "Epoch 599:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.407, train_loss_epoch=0.407, valid_loss=7.630]        \n",
      "Epoch 599: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s, v_num=0, train_loss_step=0.409, train_loss_epoch=0.407, valid_loss=7.630]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Epoch 600:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.409, train_loss_epoch=0.409, valid_loss=6.890]        \n",
      "Epoch 601:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.411, train_loss_epoch=0.411, valid_loss=6.890]        \n",
      "Epoch 602:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.419, train_loss_epoch=0.419, valid_loss=6.890]        \n",
      "Epoch 603:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.449, train_loss_epoch=0.449, valid_loss=6.890]        \n",
      "Epoch 604:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.423, train_loss_epoch=0.423, valid_loss=6.890]        \n",
      "Epoch 605:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.427, train_loss_epoch=0.427, valid_loss=6.890]        \n",
      "Epoch 606:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.424, train_loss_epoch=0.424, valid_loss=6.890]        \n",
      "Epoch 607:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.423, train_loss_epoch=0.423, valid_loss=6.890]        \n",
      "Epoch 608:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.416, train_loss_epoch=0.416, valid_loss=6.890]        \n",
      "Epoch 609:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.422, train_loss_epoch=0.422, valid_loss=6.890]        \n",
      "Epoch 610:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.413, train_loss_epoch=0.413, valid_loss=6.890]        \n",
      "Epoch 611:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.412, train_loss_epoch=0.412, valid_loss=6.890]        \n",
      "Epoch 612:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.413, train_loss_epoch=0.413, valid_loss=6.890]        \n",
      "Epoch 613:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.408, train_loss_epoch=0.408, valid_loss=6.890]        \n",
      "Epoch 614:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.406, train_loss_epoch=0.406, valid_loss=6.890]        \n",
      "Epoch 615:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.406, train_loss_epoch=0.406, valid_loss=6.890]        \n",
      "Epoch 616:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.404, train_loss_epoch=0.404, valid_loss=6.890]        \n",
      "Epoch 617:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400, valid_loss=6.890]        \n",
      "Epoch 618:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400, valid_loss=6.890]        \n",
      "Epoch 619:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.399, train_loss_epoch=0.399, valid_loss=6.890]        \n",
      "Epoch 619: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s, v_num=0, train_loss_step=0.397, train_loss_epoch=0.397, valid_loss=6.890]\n",
      "Epoch 620:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.397, train_loss_epoch=0.397, valid_loss=6.890]        \n",
      "Epoch 621:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.396, train_loss_epoch=0.396, valid_loss=6.890]        \n",
      "Epoch 622:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.394, valid_loss=6.890]        \n",
      "Epoch 623:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.393, train_loss_epoch=0.393, valid_loss=6.890]        \n",
      "Epoch 624:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.392, train_loss_epoch=0.392, valid_loss=6.890]        \n",
      "Epoch 625:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.390, train_loss_epoch=0.390, valid_loss=6.890]        \n",
      "Epoch 626:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.391, train_loss_epoch=0.391, valid_loss=6.890]        \n",
      "Epoch 627:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.388, train_loss_epoch=0.388, valid_loss=6.890]        \n",
      "Epoch 628:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.387, train_loss_epoch=0.387, valid_loss=6.890]        \n",
      "Epoch 629:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386, valid_loss=6.890]        \n",
      "Epoch 630:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.385, train_loss_epoch=0.385, valid_loss=6.890]        \n",
      "Epoch 631:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386, valid_loss=6.890]        \n",
      "Epoch 632:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.391, train_loss_epoch=0.391, valid_loss=6.890]        \n",
      "Epoch 633:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.394, valid_loss=6.890]        \n",
      "Epoch 634:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386, valid_loss=6.890]        \n",
      "Epoch 635:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381, valid_loss=6.890]        \n",
      "Epoch 636:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.383, train_loss_epoch=0.383, valid_loss=6.890]        \n",
      "Epoch 637:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386, valid_loss=6.890]        \n",
      "Epoch 638:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.383, train_loss_epoch=0.383, valid_loss=6.890]        \n",
      "Epoch 639:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.377, train_loss_epoch=0.377, valid_loss=6.890]        \n",
      "Epoch 640:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.377, train_loss_epoch=0.377, valid_loss=6.890]        \n",
      "Epoch 641:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.382, train_loss_epoch=0.382, valid_loss=6.890]        \n",
      "Epoch 642:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.379, train_loss_epoch=0.379, valid_loss=6.890]        \n",
      "Epoch 643:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.379, train_loss_epoch=0.379, valid_loss=6.890]        \n",
      "Epoch 644:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381, valid_loss=6.890]        \n",
      "Epoch 645:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381, valid_loss=6.890]        \n",
      "Epoch 646:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.380, train_loss_epoch=0.380, valid_loss=6.890]        \n",
      "Epoch 647:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.375, train_loss_epoch=0.375, valid_loss=6.890]        \n",
      "Epoch 648:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.373, train_loss_epoch=0.373, valid_loss=6.890]        \n",
      "Epoch 649:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.373, train_loss_epoch=0.373, valid_loss=6.890]        \n",
      "Epoch 650:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.369, train_loss_epoch=0.369, valid_loss=6.890]        \n",
      "Epoch 651:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.369, train_loss_epoch=0.369, valid_loss=6.890]        \n",
      "Epoch 652:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.372, train_loss_epoch=0.372, valid_loss=6.890]        \n",
      "Epoch 653:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.371, train_loss_epoch=0.371, valid_loss=6.890]        \n",
      "Epoch 654:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.374, train_loss_epoch=0.374, valid_loss=6.890]        \n",
      "Epoch 655:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.374, train_loss_epoch=0.374, valid_loss=6.890]        \n",
      "Epoch 656:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.371, train_loss_epoch=0.371, valid_loss=6.890]        \n",
      "Epoch 657:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.370, train_loss_epoch=0.370, valid_loss=6.890]        \n",
      "Epoch 658:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.366, train_loss_epoch=0.366, valid_loss=6.890]        \n",
      "Epoch 659:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.364, train_loss_epoch=0.364, valid_loss=6.890]        \n",
      "Epoch 660:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.367, train_loss_epoch=0.367, valid_loss=6.890]        \n",
      "Epoch 661:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.365, train_loss_epoch=0.365, valid_loss=6.890]        \n",
      "Epoch 662:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.364, train_loss_epoch=0.364, valid_loss=6.890]        \n",
      "Epoch 663:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.367, train_loss_epoch=0.367, valid_loss=6.890]        \n",
      "Epoch 663: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, v_num=0, train_loss_step=0.358, train_loss_epoch=0.358, valid_loss=6.890]\n",
      "Epoch 664:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.358, train_loss_epoch=0.358, valid_loss=6.890]        \n",
      "Epoch 664: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, v_num=0, train_loss_step=0.358, train_loss_epoch=0.358, valid_loss=6.890]\n",
      "Epoch 665:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.358, train_loss_epoch=0.358, valid_loss=6.890]        \n",
      "Epoch 666:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.360, train_loss_epoch=0.360, valid_loss=6.890]        \n",
      "Epoch 667:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.360, train_loss_epoch=0.360, valid_loss=6.890]        \n",
      "Epoch 668:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.358, train_loss_epoch=0.358, valid_loss=6.890]        \n",
      "Epoch 669:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359, valid_loss=6.890]        \n",
      "Epoch 670:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.361, train_loss_epoch=0.361, valid_loss=6.890]        \n",
      "Epoch 671:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.361, train_loss_epoch=0.361, valid_loss=6.890]        \n",
      "Epoch 672:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.364, train_loss_epoch=0.364, valid_loss=6.890]        \n",
      "Epoch 673:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359, valid_loss=6.890]        \n",
      "Epoch 674:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.355, train_loss_epoch=0.355, valid_loss=6.890]        \n",
      "Epoch 675:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.352, train_loss_epoch=0.352, valid_loss=6.890]        \n",
      "Epoch 676:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.353, train_loss_epoch=0.353, valid_loss=6.890]        \n",
      "Epoch 677:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.354, train_loss_epoch=0.354, valid_loss=6.890]        \n",
      "Epoch 678:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.357, train_loss_epoch=0.357, valid_loss=6.890]        \n",
      "Epoch 679:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.360, train_loss_epoch=0.360, valid_loss=6.890]        \n",
      "Epoch 680:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.354, train_loss_epoch=0.354, valid_loss=6.890]        \n",
      "Epoch 681:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.350, train_loss_epoch=0.350, valid_loss=6.890]        \n",
      "Epoch 682:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.350, train_loss_epoch=0.350, valid_loss=6.890]        \n",
      "Epoch 683:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.349, train_loss_epoch=0.349, valid_loss=6.890]        \n",
      "Epoch 684:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.349, train_loss_epoch=0.349, valid_loss=6.890]        \n",
      "Epoch 685:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.349, train_loss_epoch=0.349, valid_loss=6.890]        \n",
      "Epoch 686:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.351, train_loss_epoch=0.351, valid_loss=6.890]        \n",
      "Epoch 687:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.352, train_loss_epoch=0.352, valid_loss=6.890]        \n",
      "Epoch 688:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.348, train_loss_epoch=0.348, valid_loss=6.890]        \n",
      "Epoch 689:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.344, train_loss_epoch=0.344, valid_loss=6.890]        \n",
      "Epoch 690:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.342, train_loss_epoch=0.342, valid_loss=6.890]        \n",
      "Epoch 691:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.345, train_loss_epoch=0.345, valid_loss=6.890]        \n",
      "Epoch 692:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.344, train_loss_epoch=0.344, valid_loss=6.890]        \n",
      "Epoch 693:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.342, train_loss_epoch=0.342, valid_loss=6.890]        \n",
      "Epoch 694:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.344, train_loss_epoch=0.344, valid_loss=6.890]        \n",
      "Epoch 695:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.346, train_loss_epoch=0.346, valid_loss=6.890]        \n",
      "Epoch 696:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.342, train_loss_epoch=0.342, valid_loss=6.890]        \n",
      "Epoch 697:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.339, train_loss_epoch=0.339, valid_loss=6.890]        \n",
      "Epoch 698:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.341, train_loss_epoch=0.341, valid_loss=6.890]        \n",
      "Epoch 699:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.342, train_loss_epoch=0.342, valid_loss=6.890]        \n",
      "Epoch 699: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, v_num=0, train_loss_step=0.337, train_loss_epoch=0.342, valid_loss=6.890]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\u001b[A\n",
      "Epoch 700:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.337, train_loss_epoch=0.337, valid_loss=7.060]        \n",
      "Epoch 701:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.340, train_loss_epoch=0.340, valid_loss=7.060]        \n",
      "Epoch 702:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.341, train_loss_epoch=0.341, valid_loss=7.060]        \n",
      "Epoch 703:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.334, train_loss_epoch=0.334, valid_loss=7.060]        \n",
      "Epoch 704:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.334, train_loss_epoch=0.334, valid_loss=7.060]        \n",
      "Epoch 705:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.338, train_loss_epoch=0.338, valid_loss=7.060]        \n",
      "Epoch 706:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.333, train_loss_epoch=0.333, valid_loss=7.060]        \n",
      "Epoch 707:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331, valid_loss=7.060]        \n",
      "Epoch 708:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.336, train_loss_epoch=0.336, valid_loss=7.060]        \n",
      "Epoch 709:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.334, train_loss_epoch=0.334, valid_loss=7.060]        \n",
      "Epoch 710:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.332, train_loss_epoch=0.332, valid_loss=7.060]        \n",
      "Epoch 711:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.336, train_loss_epoch=0.336, valid_loss=7.060]        \n",
      "Epoch 712:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.333, train_loss_epoch=0.333, valid_loss=7.060]        \n",
      "Epoch 713:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.337, train_loss_epoch=0.337, valid_loss=7.060]        \n",
      "Epoch 714:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.338, train_loss_epoch=0.338, valid_loss=7.060]        \n",
      "Epoch 715:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.333, train_loss_epoch=0.333, valid_loss=7.060]        \n",
      "Epoch 716:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.333, train_loss_epoch=0.333, valid_loss=7.060]        \n",
      "Epoch 717:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331, valid_loss=7.060]        \n",
      "Epoch 718:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326, valid_loss=7.060]        \n",
      "Epoch 719:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.325, valid_loss=7.060]        \n",
      "Epoch 720:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.324, train_loss_epoch=0.324, valid_loss=7.060]        \n",
      "Epoch 721:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.324, train_loss_epoch=0.324, valid_loss=7.060]        \n",
      "Epoch 722:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326, valid_loss=7.060]        \n",
      "Epoch 723:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.328, train_loss_epoch=0.328, valid_loss=7.060]        \n",
      "Epoch 724:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331, valid_loss=7.060]        \n",
      "Epoch 725:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.332, train_loss_epoch=0.332, valid_loss=7.060]        \n",
      "Epoch 726:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.324, train_loss_epoch=0.324, valid_loss=7.060]        \n",
      "Epoch 727:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.322, train_loss_epoch=0.322, valid_loss=7.060]        \n",
      "Epoch 728:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.323, valid_loss=7.060]        \n",
      "Epoch 729:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.322, train_loss_epoch=0.322, valid_loss=7.060]        \n",
      "Epoch 730:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.324, train_loss_epoch=0.324, valid_loss=7.060]        \n",
      "Epoch 731:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326, valid_loss=7.060]        \n",
      "Epoch 732:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.319, train_loss_epoch=0.319, valid_loss=7.060]        \n",
      "Epoch 733:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.321, train_loss_epoch=0.321, valid_loss=7.060]        \n",
      "Epoch 734:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.323, valid_loss=7.060]        \n",
      "Epoch 735:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.318, train_loss_epoch=0.318, valid_loss=7.060]        \n",
      "Epoch 736:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.319, train_loss_epoch=0.319, valid_loss=7.060]        \n",
      "Epoch 737:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.318, train_loss_epoch=0.318, valid_loss=7.060]        \n",
      "Epoch 738:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.318, train_loss_epoch=0.318, valid_loss=7.060]        \n",
      "Epoch 739:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.319, train_loss_epoch=0.319, valid_loss=7.060]        \n",
      "Epoch 740:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.320, train_loss_epoch=0.320, valid_loss=7.060]        \n",
      "Epoch 741:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.323, valid_loss=7.060]        \n",
      "Epoch 742:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.321, train_loss_epoch=0.321, valid_loss=7.060]        \n",
      "Epoch 743:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.319, train_loss_epoch=0.319, valid_loss=7.060]        \n",
      "Epoch 744:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.316, valid_loss=7.060]        \n",
      "Epoch 745:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317, valid_loss=7.060]        \n",
      "Epoch 746:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317, valid_loss=7.060]        \n",
      "Epoch 747:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.316, valid_loss=7.060]        \n",
      "Epoch 748:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.316, valid_loss=7.060]        \n",
      "Epoch 749:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317, valid_loss=7.060]        \n",
      "Epoch 750:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.310, train_loss_epoch=0.310, valid_loss=7.060]        \n",
      "Epoch 751:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.312, train_loss_epoch=0.312, valid_loss=7.060]        \n",
      "Epoch 752:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.315, train_loss_epoch=0.315, valid_loss=7.060]        \n",
      "Epoch 753:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.314, train_loss_epoch=0.314, valid_loss=7.060]        \n",
      "Epoch 754:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.312, train_loss_epoch=0.312, valid_loss=7.060]        \n",
      "Epoch 755:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.311, valid_loss=7.060]        \n",
      "Epoch 756:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.310, train_loss_epoch=0.310, valid_loss=7.060]        \n",
      "Epoch 757:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.306, train_loss_epoch=0.306, valid_loss=7.060]        \n",
      "Epoch 758:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.308, train_loss_epoch=0.308, valid_loss=7.060]        \n",
      "Epoch 759:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.309, train_loss_epoch=0.309, valid_loss=7.060]        \n",
      "Epoch 760:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.312, train_loss_epoch=0.312, valid_loss=7.060]        \n",
      "Epoch 761:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.308, train_loss_epoch=0.308, valid_loss=7.060]        \n",
      "Epoch 762:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.306, train_loss_epoch=0.306, valid_loss=7.060]        \n",
      "Epoch 763:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305, valid_loss=7.060]        \n",
      "Epoch 764:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.302, train_loss_epoch=0.302, valid_loss=7.060]        \n",
      "Epoch 765:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.304, train_loss_epoch=0.304, valid_loss=7.060]        \n",
      "Epoch 766:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.303, train_loss_epoch=0.303, valid_loss=7.060]        \n",
      "Epoch 767:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.308, train_loss_epoch=0.308, valid_loss=7.060]        \n",
      "Epoch 768:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.310, train_loss_epoch=0.310, valid_loss=7.060]        \n",
      "Epoch 769:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.304, train_loss_epoch=0.304, valid_loss=7.060]        \n",
      "Epoch 770:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.301, train_loss_epoch=0.301, valid_loss=7.060]        \n",
      "Epoch 771:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.304, train_loss_epoch=0.304, valid_loss=7.060]        \n",
      "Epoch 772:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.303, train_loss_epoch=0.303, valid_loss=7.060]        \n",
      "Epoch 773:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.301, train_loss_epoch=0.301, valid_loss=7.060]        \n",
      "Epoch 774:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305, valid_loss=7.060]        \n",
      "Epoch 775:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.304, train_loss_epoch=0.304, valid_loss=7.060]        \n",
      "Epoch 776:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.301, train_loss_epoch=0.301, valid_loss=7.060]        \n",
      "Epoch 777:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.300, train_loss_epoch=0.300, valid_loss=7.060]        \n",
      "Epoch 778:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299, valid_loss=7.060]        \n",
      "Epoch 779:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.296, train_loss_epoch=0.296, valid_loss=7.060]        \n",
      "Epoch 780:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.298, train_loss_epoch=0.298, valid_loss=7.060]        \n",
      "Epoch 781:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299, valid_loss=7.060]        \n",
      "Epoch 782:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.298, train_loss_epoch=0.298, valid_loss=7.060]        \n",
      "Epoch 783:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297, valid_loss=7.060]        \n",
      "Epoch 784:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.296, train_loss_epoch=0.296, valid_loss=7.060]        \n",
      "Epoch 785:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299, valid_loss=7.060]        \n",
      "Epoch 786:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299, valid_loss=7.060]        \n",
      "Epoch 787:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297, valid_loss=7.060]        \n",
      "Epoch 788:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293, valid_loss=7.060]        \n",
      "Epoch 789:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.292, train_loss_epoch=0.292, valid_loss=7.060]        \n",
      "Epoch 790:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.294, train_loss_epoch=0.294, valid_loss=7.060]        \n",
      "Epoch 791:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295, valid_loss=7.060]        \n",
      "Epoch 792:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295, valid_loss=7.060]        \n",
      "Epoch 793:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291, valid_loss=7.060]        \n",
      "Epoch 794:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.290, train_loss_epoch=0.290, valid_loss=7.060]        \n",
      "Epoch 795:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.294, train_loss_epoch=0.294, valid_loss=7.060]        \n",
      "Epoch 796:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297, valid_loss=7.060]        \n",
      "Epoch 797:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291, valid_loss=7.060]        \n",
      "Epoch 798:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288, valid_loss=7.060]        \n",
      "Epoch 799:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291, valid_loss=7.060]        \n",
      "Epoch 799: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291, valid_loss=7.060]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\u001b[A\n",
      "Epoch 800:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291, valid_loss=7.870]        \n",
      "Epoch 801:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289, valid_loss=7.870]        \n",
      "Epoch 802:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.284, train_loss_epoch=0.284, valid_loss=7.870]        \n",
      "Epoch 803:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285, valid_loss=7.870]        \n",
      "Epoch 804:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286, valid_loss=7.870]        \n",
      "Epoch 805:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285, valid_loss=7.870]        \n",
      "Epoch 806:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286, valid_loss=7.870]        \n",
      "Epoch 807:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289, valid_loss=7.870]        \n",
      "Epoch 808:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.290, train_loss_epoch=0.290, valid_loss=7.870]        \n",
      "Epoch 809:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293, valid_loss=7.870]        \n",
      "Epoch 810:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285, valid_loss=7.870]        \n",
      "Epoch 811:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283, valid_loss=7.870]        \n",
      "Epoch 812:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283, valid_loss=7.870]        \n",
      "Epoch 813:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.284, train_loss_epoch=0.284, valid_loss=7.870]        \n",
      "Epoch 814:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283, valid_loss=7.870]        \n",
      "Epoch 815:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283, valid_loss=7.870]        \n",
      "Epoch 816:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286, valid_loss=7.870]        \n",
      "Epoch 817:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.287, train_loss_epoch=0.287, valid_loss=7.870]        \n",
      "Epoch 818:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.284, train_loss_epoch=0.284, valid_loss=7.870]        \n",
      "Epoch 819:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.282, train_loss_epoch=0.282, valid_loss=7.870]        \n",
      "Epoch 820:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.278, train_loss_epoch=0.278, valid_loss=7.870]        \n",
      "Epoch 821:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280, valid_loss=7.870]        \n",
      "Epoch 822:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.282, train_loss_epoch=0.282, valid_loss=7.870]        \n",
      "Epoch 823:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283, valid_loss=7.870]        \n",
      "Epoch 824:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279, valid_loss=7.870]        \n",
      "Epoch 825:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279, valid_loss=7.870]        \n",
      "Epoch 826:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279, valid_loss=7.870]        \n",
      "Epoch 827:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279, valid_loss=7.870]        \n",
      "Epoch 828:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280, valid_loss=7.870]        \n",
      "Epoch 829:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.281, train_loss_epoch=0.281, valid_loss=7.870]        \n",
      "Epoch 830:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.278, train_loss_epoch=0.278, valid_loss=7.870]        \n",
      "Epoch 831:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279, valid_loss=7.870]        \n",
      "Epoch 832:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=7.870]        \n",
      "Epoch 832: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=7.870]\n",
      "Epoch 833:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=7.870]        \n",
      "Epoch 834:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=7.870]        \n",
      "Epoch 835:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.273, train_loss_epoch=0.273, valid_loss=7.870]        \n",
      "Epoch 836:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=7.870]        \n",
      "Epoch 837:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.278, train_loss_epoch=0.278, valid_loss=7.870]        \n",
      "Epoch 838:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=7.870]        \n",
      "Epoch 839:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.273, train_loss_epoch=0.273, valid_loss=7.870]        \n",
      "Epoch 840:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.277, train_loss_epoch=0.277, valid_loss=7.870]        \n",
      "Epoch 841:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.277, train_loss_epoch=0.277, valid_loss=7.870]        \n",
      "Epoch 842:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.274, train_loss_epoch=0.274, valid_loss=7.870]        \n",
      "Epoch 843:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=7.870]        \n",
      "Epoch 844:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.274, train_loss_epoch=0.274, valid_loss=7.870]        \n",
      "Epoch 845:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.271, train_loss_epoch=0.271, valid_loss=7.870]        \n",
      "Epoch 846:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268, valid_loss=7.870]        \n",
      "Epoch 847:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268, valid_loss=7.870]        \n",
      "Epoch 848:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.272, train_loss_epoch=0.272, valid_loss=7.870]        \n",
      "Epoch 849:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.272, train_loss_epoch=0.272, valid_loss=7.870]        \n",
      "Epoch 850:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.273, train_loss_epoch=0.273, valid_loss=7.870]        \n",
      "Epoch 851:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.272, train_loss_epoch=0.272, valid_loss=7.870]        \n",
      "Epoch 852:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.269, train_loss_epoch=0.269, valid_loss=7.870]        \n",
      "Epoch 853:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268, valid_loss=7.870]        \n",
      "Epoch 854:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.271, train_loss_epoch=0.271, valid_loss=7.870]        \n",
      "Epoch 855:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.271, train_loss_epoch=0.271, valid_loss=7.870]        \n",
      "Epoch 856:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267, valid_loss=7.870]        \n",
      "Epoch 857:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264, valid_loss=7.870]        \n",
      "Epoch 858:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.266, train_loss_epoch=0.266, valid_loss=7.870]        \n",
      "Epoch 859:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267, valid_loss=7.870]        \n",
      "Epoch 860:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.269, train_loss_epoch=0.269, valid_loss=7.870]        \n",
      "Epoch 861:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264, valid_loss=7.870]        \n",
      "Epoch 862:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263, valid_loss=7.870]        \n",
      "Epoch 863:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.265, train_loss_epoch=0.265, valid_loss=7.870]        \n",
      "Epoch 864:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.265, train_loss_epoch=0.265, valid_loss=7.870]        \n",
      "Epoch 865:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264, valid_loss=7.870]        \n",
      "Epoch 866:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264, valid_loss=7.870]        \n",
      "Epoch 867:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.266, train_loss_epoch=0.266, valid_loss=7.870]        \n",
      "Epoch 868:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267, valid_loss=7.870]        \n",
      "Epoch 869:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268, valid_loss=7.870]        \n",
      "Epoch 870:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263, valid_loss=7.870]        \n",
      "Epoch 871:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.259, valid_loss=7.870]        \n",
      "Epoch 872:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260, valid_loss=7.870]        \n",
      "Epoch 873:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260, valid_loss=7.870]        \n",
      "Epoch 874:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.259, valid_loss=7.870]        \n",
      "Epoch 875:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.259, valid_loss=7.870]        \n",
      "Epoch 876:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260, valid_loss=7.870]        \n",
      "Epoch 877:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262, valid_loss=7.870]        \n",
      "Epoch 878:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.261, train_loss_epoch=0.261, valid_loss=7.870]        \n",
      "Epoch 879:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260, valid_loss=7.870]        \n",
      "Epoch 880:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.258, train_loss_epoch=0.258, valid_loss=7.870]        \n",
      "Epoch 881:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.257, train_loss_epoch=0.257, valid_loss=7.870]        \n",
      "Epoch 882:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.259, valid_loss=7.870]        \n",
      "Epoch 883:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=7.870]        \n",
      "Epoch 884:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=7.870]        \n",
      "Epoch 885:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255, valid_loss=7.870]        \n",
      "Epoch 886:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.258, train_loss_epoch=0.258, valid_loss=7.870]        \n",
      "Epoch 887:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263, valid_loss=7.870]        \n",
      "Epoch 888:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260, valid_loss=7.870]        \n",
      "Epoch 889:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=7.870]        \n",
      "Epoch 890:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=7.870]        \n",
      "Epoch 891:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.258, train_loss_epoch=0.258, valid_loss=7.870]        \n",
      "Epoch 892:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=7.870]        \n",
      "Epoch 893:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254, valid_loss=7.870]        \n",
      "Epoch 894:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=7.870]        \n",
      "Epoch 895:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255, valid_loss=7.870]        \n",
      "Epoch 896:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.251, train_loss_epoch=0.251, valid_loss=7.870]        \n",
      "Epoch 897:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.249, train_loss_epoch=0.249, valid_loss=7.870]        \n",
      "Epoch 898:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252, valid_loss=7.870]        \n",
      "Epoch 899:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=7.870]        \n",
      "Epoch 899: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.256, valid_loss=7.870]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\u001b[A\n",
      "Epoch 900:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255, valid_loss=7.730]        \n",
      "Epoch 901:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.253, train_loss_epoch=0.253, valid_loss=7.730]        \n",
      "Epoch 902:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252, valid_loss=7.730]        \n",
      "Epoch 903:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.253, train_loss_epoch=0.253, valid_loss=7.730]        \n",
      "Epoch 904:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252, valid_loss=7.730]        \n",
      "Epoch 905:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.253, train_loss_epoch=0.253, valid_loss=7.730]        \n",
      "Epoch 906:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.253, train_loss_epoch=0.253, valid_loss=7.730]        \n",
      "Epoch 907:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248, valid_loss=7.730]        \n",
      "Epoch 908:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248, valid_loss=7.730]        \n",
      "Epoch 909:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.249, train_loss_epoch=0.249, valid_loss=7.730]        \n",
      "Epoch 910:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.249, train_loss_epoch=0.249, valid_loss=7.730]        \n",
      "Epoch 911:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247, valid_loss=7.730]        \n",
      "Epoch 912:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248, valid_loss=7.730]        \n",
      "Epoch 913:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.251, train_loss_epoch=0.251, valid_loss=7.730]        \n",
      "Epoch 914:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250, valid_loss=7.730]        \n",
      "Epoch 915:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.246, train_loss_epoch=0.246, valid_loss=7.730]        \n",
      "Epoch 916:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247, valid_loss=7.730]        \n",
      "Epoch 917:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252, valid_loss=7.730]        \n",
      "Epoch 918:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250, valid_loss=7.730]        \n",
      "Epoch 919:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=7.730]        \n",
      "Epoch 920:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242, valid_loss=7.730]        \n",
      "Epoch 921:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=7.730]        \n",
      "Epoch 922:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.246, train_loss_epoch=0.246, valid_loss=7.730]        \n",
      "Epoch 923:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247, valid_loss=7.730]        \n",
      "Epoch 924:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=7.730]        \n",
      "Epoch 925:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.243, train_loss_epoch=0.243, valid_loss=7.730]        \n",
      "Epoch 926:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=7.730]        \n",
      "Epoch 927:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=7.730]        \n",
      "Epoch 928:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=7.730]        \n",
      "Epoch 929:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.243, train_loss_epoch=0.243, valid_loss=7.730]        \n",
      "Epoch 930:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=7.730]        \n",
      "Epoch 931:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=7.730]        \n",
      "Epoch 932:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=7.730]        \n",
      "Epoch 933:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242, valid_loss=7.730]        \n",
      "Epoch 934:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=7.730]        \n",
      "Epoch 934: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s, v_num=0, train_loss_step=0.243, train_loss_epoch=0.243, valid_loss=7.730]\n",
      "Epoch 935:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.243, train_loss_epoch=0.243, valid_loss=7.730]        \n",
      "Epoch 936:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=7.730]        \n",
      "Epoch 937:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242, valid_loss=7.730]        \n",
      "Epoch 938:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240, valid_loss=7.730]        \n",
      "Epoch 939:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240, valid_loss=7.730]        \n",
      "Epoch 940:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239, valid_loss=7.730]        \n",
      "Epoch 941:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237, valid_loss=7.730]        \n",
      "Epoch 942:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=7.730]        \n",
      "Epoch 943:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239, valid_loss=7.730]        \n",
      "Epoch 944:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239, valid_loss=7.730]        \n",
      "Epoch 945:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240, valid_loss=7.730]        \n",
      "Epoch 946:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239, valid_loss=7.730]        \n",
      "Epoch 947:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.238, train_loss_epoch=0.238, valid_loss=7.730]        \n",
      "Epoch 948:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.238, train_loss_epoch=0.238, valid_loss=7.730]        \n",
      "Epoch 949:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=7.730]        \n",
      "Epoch 950:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.235, valid_loss=7.730]        \n",
      "Epoch 951:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]        \n",
      "Epoch 952:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234, valid_loss=7.730]        \n",
      "Epoch 953:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237, valid_loss=7.730]        \n",
      "Epoch 954:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237, valid_loss=7.730]        \n",
      "Epoch 955:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.235, valid_loss=7.730]        \n",
      "Epoch 956:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232, valid_loss=7.730]        \n",
      "Epoch 957:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232, valid_loss=7.730]        \n",
      "Epoch 958:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234, valid_loss=7.730]        \n",
      "Epoch 959:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237, valid_loss=7.730]        \n",
      "Epoch 960:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240, valid_loss=7.730]        \n",
      "Epoch 961:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239, valid_loss=7.730]        \n",
      "Epoch 962:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233, valid_loss=7.730]        \n",
      "Epoch 963:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233, valid_loss=7.730]        \n",
      "Epoch 964:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233, valid_loss=7.730]        \n",
      "Epoch 964: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]\n",
      "Epoch 965:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]        \n",
      "Epoch 966:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]        \n",
      "Epoch 967:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]        \n",
      "Epoch 968:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.230, train_loss_epoch=0.230, valid_loss=7.730]        \n",
      "Epoch 969:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]        \n",
      "Epoch 970:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]        \n",
      "Epoch 971:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232, valid_loss=7.730]        \n",
      "Epoch 972:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232, valid_loss=7.730]        \n",
      "Epoch 973:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231, valid_loss=7.730]        \n",
      "Epoch 974:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232, valid_loss=7.730]        \n",
      "Epoch 975:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233, valid_loss=7.730]        \n",
      "Epoch 976:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233, valid_loss=7.730]        \n",
      "Epoch 977:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.228, valid_loss=7.730]        \n",
      "Epoch 978:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=7.730]        \n",
      "Epoch 979:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227, valid_loss=7.730]        \n",
      "Epoch 980:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.228, valid_loss=7.730]        \n",
      "Epoch 981:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227, valid_loss=7.730]        \n",
      "Epoch 982:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=7.730]        \n",
      "Epoch 983:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=7.730]        \n",
      "Epoch 984:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225, valid_loss=7.730]        \n",
      "Epoch 985:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=7.730]        \n",
      "Epoch 986:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.228, valid_loss=7.730]        \n",
      "Epoch 987:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227, valid_loss=7.730]        \n",
      "Epoch 988:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=7.730]        \n",
      "Epoch 989:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.229, train_loss_epoch=0.229, valid_loss=7.730]        \n",
      "Epoch 990:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.228, valid_loss=7.730]        \n",
      "Epoch 991:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.224, train_loss_epoch=0.224, valid_loss=7.730]        \n",
      "Epoch 992:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.224, train_loss_epoch=0.224, valid_loss=7.730]        \n",
      "Epoch 993:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=7.730]        \n",
      "Epoch 994:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227, valid_loss=7.730]        \n",
      "Epoch 995:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.221, valid_loss=7.730]        \n",
      "Epoch 996:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.224, train_loss_epoch=0.224, valid_loss=7.730]        \n",
      "Epoch 997:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227, valid_loss=7.730]        \n",
      "Epoch 998:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.224, train_loss_epoch=0.224, valid_loss=7.730]        \n",
      "Epoch 999:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226, valid_loss=7.730]        \n",
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.226, valid_loss=7.730]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=3332)\u001b[0m `Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=3332)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\u001b[A\n",
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.221, valid_loss=8.090]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=28888)\u001b[0m C:\\Users\\35841\\Desktop\\energy_consumption_modeling\\energyEV\\Lib\\site-packages\\ray\\tune\\integration\\pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m Seed set to 3\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m \n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m   | Name            | Type          | Params | Mode \n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 0 | loss            | MAE           | 0      | train\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 1 | padder          | ConstantPad1d | 0      | train\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 2 | scaler          | TemporalNorm  | 0      | train\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 3 | hist_encoder    | LSTM          | 1.8 M  | train\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 4 | context_adapter | Linear        | 903 K  | train\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 5 | mlp_decoder     | MLP           | 13.3 K | train\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 2.7 M     Trainable params\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 2.7 M     Total params\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 10.899    Total estimated model params size (MB)\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 11        Modules in train mode\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.832, train_loss_epoch=0.832]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.945, train_loss_epoch=0.945]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.804, train_loss_epoch=0.804]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.973, train_loss_epoch=0.973]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810]        \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.799, train_loss_epoch=0.799]        \n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  9.10it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.915, train_loss_epoch=0.915]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.220, train_loss_epoch=1.220]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.829, train_loss_epoch=0.829]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.893, train_loss_epoch=0.893]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.390, train_loss_epoch=1.390]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.790, train_loss_epoch=0.790]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140]        \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.310, train_loss_epoch=1.310]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.350, train_loss_epoch=1.350]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.420, train_loss_epoch=1.420]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894]        \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.320, train_loss_epoch=1.320]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.998, train_loss_epoch=0.998]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870]        \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870]\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.290, train_loss_epoch=1.290]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.988, train_loss_epoch=0.988]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.410, train_loss_epoch=1.410]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.854, train_loss_epoch=0.854]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.260, train_loss_epoch=1.260]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.988, train_loss_epoch=0.988]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.996]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00,  9.77it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170]\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.874, train_loss_epoch=0.874]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.807, train_loss_epoch=0.807]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.989, train_loss_epoch=0.989]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.794, train_loss_epoch=0.794]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.370, train_loss_epoch=1.370]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.978, train_loss_epoch=0.978]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881]        \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.400, train_loss_epoch=1.400]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.851]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=10.70]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.70]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=10.70]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.70]        \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=10.70]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.923, train_loss_epoch=0.923, valid_loss=10.70]        \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.901, train_loss_epoch=0.901, valid_loss=10.70]        \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.70]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=10.70]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.70]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.831, train_loss_epoch=0.831, valid_loss=10.70]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994, valid_loss=10.70]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=10.70]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.70]        \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=10.70]        \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=10.70]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.984, train_loss_epoch=0.984, valid_loss=10.70]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=10.70]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843, valid_loss=10.70]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=10.70]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.768, train_loss_epoch=0.768, valid_loss=10.70]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=10.70]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=10.70]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.966, train_loss_epoch=0.966, valid_loss=10.70]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.410, train_loss_epoch=1.410, valid_loss=10.70]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200, valid_loss=10.70]        \n",
      "Epoch 127: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=1.200, valid_loss=10.70]\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911, valid_loss=10.70]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.250, valid_loss=10.70]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.370, train_loss_epoch=1.370, valid_loss=10.70]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210, valid_loss=10.70]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.995, train_loss_epoch=0.995, valid_loss=10.70]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=10.70]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=10.70]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.70]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=10.70]        \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=10.70]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=10.70]        \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.180, train_loss_epoch=1.180, valid_loss=10.70]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=10.70]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.790, train_loss_epoch=0.790, valid_loss=10.70]        \n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843, valid_loss=10.70]\n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843, valid_loss=10.70]        \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.681, train_loss_epoch=0.681, valid_loss=10.70]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=10.70]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=10.70]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798, valid_loss=10.70]        \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=10.70]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.70]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.70]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.908, train_loss_epoch=0.908, valid_loss=10.70]        \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.70]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.775, train_loss_epoch=0.775, valid_loss=10.70]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=10.70]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.785, train_loss_epoch=0.785, valid_loss=10.70]        \n",
      "Epoch 157: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=10.70]\n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=10.70]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.734, train_loss_epoch=0.734, valid_loss=10.70]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130, valid_loss=10.70]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.779, train_loss_epoch=0.779, valid_loss=10.70]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.859, train_loss_epoch=0.859, valid_loss=10.70]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885, valid_loss=10.70]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.915, train_loss_epoch=0.915, valid_loss=10.70]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=10.70]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.70]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.998, train_loss_epoch=0.998, valid_loss=10.70]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=10.70]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=10.70]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=10.70]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=10.70]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=10.70]        \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.713, train_loss_epoch=0.713, valid_loss=10.70]        \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911, valid_loss=10.70]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=10.70]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.914, valid_loss=10.70]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.938, train_loss_epoch=0.938, valid_loss=10.70]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=10.70]        \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=10.70]\n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=10.70]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809, valid_loss=10.70]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.838, train_loss_epoch=0.838, valid_loss=10.70]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.980, train_loss_epoch=0.980, valid_loss=10.70]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=10.70]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=10.70]        \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.918, train_loss_epoch=0.918, valid_loss=10.70]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.690, train_loss_epoch=0.690, valid_loss=10.70]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=10.70]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.731, train_loss_epoch=0.731, valid_loss=10.70]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950, valid_loss=10.70]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.70]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=10.70]\n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.882, train_loss_epoch=0.882, valid_loss=10.70]        \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940, valid_loss=10.70]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=10.70]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.740, valid_loss=10.70]        \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=10.70]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.969, valid_loss=10.70]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=14.10]        \n",
      "Epoch 200: 100%|██████████| 1/1 [00:00<00:00,  8.38it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230, valid_loss=14.10]\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230, valid_loss=14.10]        \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.350, train_loss_epoch=1.350, valid_loss=14.10]        \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.846, train_loss_epoch=0.846, valid_loss=14.10]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=14.10]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=14.10]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.973, train_loss_epoch=0.973, valid_loss=14.10]        \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.873, valid_loss=14.10]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.993, train_loss_epoch=0.993, valid_loss=14.10]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150, valid_loss=14.10]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.890, train_loss_epoch=0.890, valid_loss=14.10]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=14.10]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.250, train_loss_epoch=1.250, valid_loss=14.10]        \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=14.10]        \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=14.10]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.750, train_loss_epoch=0.750, valid_loss=14.10]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=14.10]        \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=14.10]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.992, train_loss_epoch=0.992, valid_loss=14.10]        \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=14.10]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953, valid_loss=14.10]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.787, train_loss_epoch=0.787, valid_loss=14.10]        \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.785, train_loss_epoch=0.785, valid_loss=14.10]        \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851, valid_loss=14.10]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130, valid_loss=14.10]        \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.844, train_loss_epoch=0.844, valid_loss=14.10]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=14.10]        \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=14.10]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.831, train_loss_epoch=0.831, valid_loss=14.10]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.795, train_loss_epoch=0.795, valid_loss=14.10]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=14.10]        \n",
      "Epoch 231: 100%|██████████| 1/1 [00:00<00:00, 11.68it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=14.10]\n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.844, train_loss_epoch=0.844, valid_loss=14.10]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=14.10]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=14.10]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=14.10]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=14.10]        \n",
      "Epoch 236: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.020, valid_loss=14.10]\n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=14.10]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=14.10]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=14.10]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.829, train_loss_epoch=0.829, valid_loss=14.10]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=14.10]        \n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.801, train_loss_epoch=0.801, valid_loss=14.10]        \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=14.10]        \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=14.10]        \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=14.10]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.918, train_loss_epoch=0.918, valid_loss=14.10]        \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816, valid_loss=14.10]        \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=14.10]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=14.10]        \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=1.110, valid_loss=14.10]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683, valid_loss=14.10]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.796, train_loss_epoch=0.796, valid_loss=14.10]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=14.10]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.340, train_loss_epoch=1.340, valid_loss=14.10]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.861, train_loss_epoch=0.861, valid_loss=14.10]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=14.10]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=14.10]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=14.10]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809, valid_loss=14.10]        \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=14.10]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=14.10]        \n",
      "Epoch 261: 100%|██████████| 1/1 [00:00<00:00, 10.61it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=14.10]\n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=14.10]        \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.701, train_loss_epoch=0.701, valid_loss=14.10]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.842, train_loss_epoch=0.842, valid_loss=14.10]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=14.10]        \n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=14.10]        \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851, valid_loss=14.10]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=14.10]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=14.10]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.968, train_loss_epoch=0.968, valid_loss=14.10]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=14.10]        \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.782, train_loss_epoch=0.782, valid_loss=14.10]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994, valid_loss=14.10]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.090, train_loss_epoch=1.090, valid_loss=14.10]        \n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997, valid_loss=14.10]        \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857, valid_loss=14.10]        \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=14.10]        \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=14.10]        \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.958, train_loss_epoch=0.958, valid_loss=14.10]        \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=14.10]        \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=14.10]        \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909, valid_loss=14.10]        \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=14.10]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=14.10]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.686, train_loss_epoch=0.686, valid_loss=14.10]        \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=14.10]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888, valid_loss=14.10]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.758, train_loss_epoch=0.758, valid_loss=14.10]        \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.823, train_loss_epoch=0.823, valid_loss=14.10]        \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.796, train_loss_epoch=0.796, valid_loss=14.10]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=14.10]        \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.662, train_loss_epoch=0.662, valid_loss=14.10]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.634, train_loss_epoch=0.634, valid_loss=14.10]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851, valid_loss=14.10]        \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.841, train_loss_epoch=0.841, valid_loss=14.10]        \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=14.10]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.829, train_loss_epoch=0.829, valid_loss=14.10]        \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.788, train_loss_epoch=0.788, valid_loss=14.10]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.788, valid_loss=14.10]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\u001b[A\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=12.00]        \n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=12.00]        \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=12.00]        \n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.675, train_loss_epoch=0.675, valid_loss=12.00]        \n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.786, valid_loss=12.00]        \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.763, train_loss_epoch=0.763, valid_loss=12.00]        \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=12.00]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.756, train_loss_epoch=0.756, valid_loss=12.00]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.677, train_loss_epoch=0.677, valid_loss=12.00]        \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.280, train_loss_epoch=1.280, valid_loss=12.00]        \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929, valid_loss=12.00]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810, valid_loss=12.00]        \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=12.00]        \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=12.00]        \n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=12.00]        \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.866, train_loss_epoch=0.866, valid_loss=12.00]        \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=12.00]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=12.00]        \n",
      "Epoch 319: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=12.00]\n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=12.00]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=12.00]        \n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.923, train_loss_epoch=0.923, valid_loss=12.00]        \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.733, train_loss_epoch=0.733, valid_loss=12.00]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.915, train_loss_epoch=0.915, valid_loss=12.00]        \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.814, train_loss_epoch=0.814, valid_loss=12.00]        \n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.702, train_loss_epoch=0.702, valid_loss=12.00]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=12.00]        \n",
      "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.756, train_loss_epoch=0.756, valid_loss=12.00]        \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.698, train_loss_epoch=0.698, valid_loss=12.00]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.964, train_loss_epoch=0.964, valid_loss=12.00]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935, valid_loss=12.00]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=12.00]        \n",
      "Epoch 334: 100%|██████████| 1/1 [00:00<00:00,  9.89it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=12.00]\n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=12.00]        \n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.977, train_loss_epoch=0.977, valid_loss=12.00]        \n",
      "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=12.00]        \n",
      "Epoch 338:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=12.00]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=12.00]        \n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.753, train_loss_epoch=0.753, valid_loss=12.00]        \n",
      "Epoch 341: 100%|██████████| 1/1 [00:00<00:00, 10.85it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=12.00]\n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=12.00]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.723, train_loss_epoch=0.723, valid_loss=12.00]        \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.815, train_loss_epoch=0.815, valid_loss=12.00]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837, valid_loss=12.00]        \n",
      "Epoch 346: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=12.00]\n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=12.00]        \n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=12.00]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808, valid_loss=12.00]        \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=12.00]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.752, train_loss_epoch=0.752, valid_loss=12.00]        \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=12.00]        \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=12.00]        \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.819, train_loss_epoch=0.819, valid_loss=12.00]        \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.779, train_loss_epoch=0.779, valid_loss=12.00]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=12.00]        \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=12.00]        \n",
      "Epoch 357: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=12.00]\n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=12.00]        \n",
      "Epoch 359:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.868, train_loss_epoch=0.868, valid_loss=12.00]        \n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994, valid_loss=12.00]        \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.763, train_loss_epoch=0.763, valid_loss=12.00]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=12.00]        \n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=12.00]        \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.807, train_loss_epoch=0.807, valid_loss=12.00]        \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955, valid_loss=12.00]        \n",
      "Epoch 366:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.871, train_loss_epoch=0.871, valid_loss=12.00]        \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.690, train_loss_epoch=0.690, valid_loss=12.00]        \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=12.00]        \n",
      "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.771, train_loss_epoch=0.771, valid_loss=12.00]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.846, train_loss_epoch=0.846, valid_loss=12.00]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230, valid_loss=12.00]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=12.00]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.831, train_loss_epoch=0.831, valid_loss=12.00]        \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=12.00]        \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=12.00]        \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942, valid_loss=12.00]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.914, valid_loss=12.00]        \n",
      "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=12.00]        \n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=12.00]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.841, train_loss_epoch=0.841, valid_loss=12.00]        \n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.817, train_loss_epoch=0.817, valid_loss=12.00]        \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839, valid_loss=12.00]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.836, train_loss_epoch=0.836, valid_loss=12.00]        \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.977, train_loss_epoch=0.977, valid_loss=12.00]        \n",
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=12.00]        \n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=12.00]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=12.00]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.741, train_loss_epoch=0.741, valid_loss=12.00]        \n",
      "Epoch 389: 100%|██████████| 1/1 [00:00<00:00,  9.71it/s, v_num=0, train_loss_step=0.735, train_loss_epoch=0.735, valid_loss=12.00]\n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.735, train_loss_epoch=0.735, valid_loss=12.00]        \n",
      "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.780, train_loss_epoch=0.780, valid_loss=12.00]        \n",
      "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.684, train_loss_epoch=0.684, valid_loss=12.00]        \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=12.00]        \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.758, train_loss_epoch=0.758, valid_loss=12.00]        \n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=12.00]        \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.829, train_loss_epoch=0.829, valid_loss=12.00]        \n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.897, train_loss_epoch=0.897, valid_loss=12.00]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.938, train_loss_epoch=0.938, valid_loss=12.00]        \n",
      "Epoch 399:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.948, train_loss_epoch=0.948, valid_loss=12.00]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.948, valid_loss=12.00]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=28888)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\u001b[A\n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851, valid_loss=7.740]        \n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.873, valid_loss=7.740]        \n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=7.740]        \n",
      "Epoch 403:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.849, train_loss_epoch=0.849, valid_loss=7.740]        \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=7.740]        \n",
      "Epoch 405:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=7.740]        \n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=7.740]        \n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887, valid_loss=7.740]        \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.874, train_loss_epoch=0.874, valid_loss=7.740]        \n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.666, train_loss_epoch=0.666, valid_loss=7.740]        \n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.873, valid_loss=7.740]        \n",
      "Epoch 411:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.789, train_loss_epoch=0.789, valid_loss=7.740]        \n",
      "Epoch 412:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.864, train_loss_epoch=0.864, valid_loss=7.740]        \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.766, train_loss_epoch=0.766, valid_loss=7.740]        \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.764, train_loss_epoch=0.764, valid_loss=7.740]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837, valid_loss=7.740]        \n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.819, train_loss_epoch=0.819, valid_loss=7.740]        \n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808, valid_loss=7.740]        \n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.777, train_loss_epoch=0.777, valid_loss=7.740]        \n",
      "Epoch 420:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.826, train_loss_epoch=0.826, valid_loss=7.740]        \n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810, valid_loss=7.740]        \n",
      "Epoch 422:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=7.740]        \n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=7.740]        \n",
      "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.692, train_loss_epoch=0.692, valid_loss=7.740]        \n",
      "Epoch 425:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=7.740]        \n",
      "Epoch 426:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.814, train_loss_epoch=0.814, valid_loss=7.740]        \n",
      "Epoch 427:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=7.740]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985, valid_loss=7.740]        \n",
      "Epoch 429:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.854, train_loss_epoch=0.854, valid_loss=7.740]        \n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.753, train_loss_epoch=0.753, valid_loss=7.740]        \n",
      "Epoch 431:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.788, train_loss_epoch=0.788, valid_loss=7.740]        \n",
      "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798, valid_loss=7.740]        \n",
      "Epoch 433:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.914, valid_loss=7.740]        \n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=7.740]        \n",
      "Epoch 435:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.637, train_loss_epoch=0.637, valid_loss=7.740]        \n",
      "Epoch 436:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.630, train_loss_epoch=0.630, valid_loss=7.740]        \n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.708, train_loss_epoch=0.708, valid_loss=7.740]        \n",
      "Epoch 438:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=7.740]        \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798, valid_loss=7.740]        \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867, valid_loss=7.740]        \n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=7.740]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.846, train_loss_epoch=0.846, valid_loss=7.740]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.973, train_loss_epoch=0.973, valid_loss=7.740]        \n",
      "Epoch 444:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.783, train_loss_epoch=0.783, valid_loss=7.740]        \n",
      "Epoch 445:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887, valid_loss=7.740]        \n",
      "Epoch 446:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.860, train_loss_epoch=0.860, valid_loss=7.740]        \n",
      "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.925, train_loss_epoch=0.925, valid_loss=7.740]        \n",
      "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.828, train_loss_epoch=0.828, valid_loss=7.740]        \n",
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=7.740]        \n",
      "Epoch 450:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816, valid_loss=7.740]        \n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=7.740]        \n",
      "Epoch 452:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=7.740]        \n",
      "Epoch 453:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837, valid_loss=7.740]        \n",
      "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885, valid_loss=7.740]        \n",
      "Epoch 455:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.784, train_loss_epoch=0.784, valid_loss=7.740]        \n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839, valid_loss=7.740]        \n",
      "Epoch 457:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.855, train_loss_epoch=0.855, valid_loss=7.740]        \n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.763, train_loss_epoch=0.763, valid_loss=7.740]        \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.793, train_loss_epoch=0.793, valid_loss=7.740]        \n",
      "Epoch 459: 100%|██████████| 1/1 [00:00<00:00,  9.55it/s, v_num=0, train_loss_step=0.706, train_loss_epoch=0.793, valid_loss=7.740]\n",
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.706, train_loss_epoch=0.706, valid_loss=7.740]        \n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.764, train_loss_epoch=0.764, valid_loss=7.740]        \n",
      "Epoch 462:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.745, train_loss_epoch=0.745, valid_loss=7.740]        \n",
      "Epoch 463:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.897, train_loss_epoch=0.897, valid_loss=7.740]        \n",
      "Epoch 464:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.802, train_loss_epoch=0.802, valid_loss=7.740]        \n",
      "Epoch 465: 100%|██████████| 1/1 [00:00<00:00, 11.19it/s, v_num=0, train_loss_step=0.995, train_loss_epoch=0.995, valid_loss=7.740]\n",
      "Epoch 466:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.772, train_loss_epoch=0.772, valid_loss=7.740]        \n",
      "Epoch 467:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=7.740]        \n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.742, train_loss_epoch=0.742, valid_loss=7.740]        \n",
      "Epoch 469:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=7.740]        \n",
      "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.840, train_loss_epoch=0.840, valid_loss=7.740]        \n",
      "Epoch 471:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.631, train_loss_epoch=0.631, valid_loss=7.740]        \n",
      "Epoch 472:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.719, train_loss_epoch=0.719, valid_loss=7.740]        \n",
      "Epoch 473:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.749, train_loss_epoch=0.749, valid_loss=7.740]        \n",
      "Epoch 474:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939, valid_loss=7.740]        \n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.643, train_loss_epoch=0.643, valid_loss=7.740]        \n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.937, train_loss_epoch=0.937, valid_loss=7.740]        \n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.747, train_loss_epoch=0.747, valid_loss=7.740]        \n",
      "Epoch 478:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.765, train_loss_epoch=0.765, valid_loss=7.740]        \n",
      "Epoch 479:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.789, train_loss_epoch=0.789, valid_loss=7.740]        \n",
      "Epoch 480:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=7.740]        \n",
      "Epoch 481:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.768, train_loss_epoch=0.768, valid_loss=7.740]        \n",
      "Epoch 482:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=7.740]        \n",
      "Epoch 483:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857, valid_loss=7.740]        \n",
      "Epoch 484:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=7.740]        \n",
      "Epoch 485:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.729, train_loss_epoch=0.729, valid_loss=7.740]        \n",
      "Epoch 486:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.993, train_loss_epoch=0.993, valid_loss=7.740]        \n",
      "Epoch 487:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=7.740]        \n",
      "Epoch 488:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.871, train_loss_epoch=0.871, valid_loss=7.740]        \n",
      "Epoch 489:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.710, train_loss_epoch=0.710, valid_loss=7.740]        \n",
      "Epoch 490:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.848, train_loss_epoch=0.848, valid_loss=7.740]        \n",
      "Epoch 491:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.730, train_loss_epoch=0.730, valid_loss=7.740]        \n",
      "Epoch 492:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.801, train_loss_epoch=0.801, valid_loss=7.740]        \n",
      "Epoch 493:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.871, train_loss_epoch=0.871, valid_loss=7.740]        \n",
      "Epoch 494:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=7.740]        \n",
      "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.792, train_loss_epoch=0.792, valid_loss=7.740]        \n",
      "Epoch 497:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.754, train_loss_epoch=0.754, valid_loss=7.740]        \n",
      "Epoch 498:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.642, train_loss_epoch=0.642, valid_loss=7.740]        \n",
      "Epoch 499:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=7.740]        \n",
      "Epoch 499: 100%|██████████| 1/1 [00:00<00:00,  8.03it/s, v_num=0, train_loss_step=0.773, train_loss_epoch=0.800, valid_loss=7.740]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=28888)\u001b[0m `Trainer.fit` stopped: `max_steps=500` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=28888)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\u001b[A\n",
      "Epoch 499: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s, v_num=0, train_loss_step=0.773, train_loss_epoch=0.773, valid_loss=9.290]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=7520)\u001b[0m C:\\Users\\35841\\Desktop\\energy_consumption_modeling\\energyEV\\Lib\\site-packages\\ray\\tune\\integration\\pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m Seed set to 10\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m   | Name            | Type          | Params | Mode \n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 0 | loss            | MAE           | 0      | train\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 1 | padder          | ConstantPad1d | 0      | train\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 2 | scaler          | TemporalNorm  | 0      | train\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 3 | hist_encoder    | LSTM          | 31.0 K | train\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 4 | context_adapter | Linear        | 153 K  | train\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 5 | mlp_decoder     | MLP           | 3.3 K  | train\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 187 K     Trainable params\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 187 K     Total params\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 0.749     Total estimated model params size (MB)\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 11        Modules in train mode\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.825, train_loss_epoch=0.825]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.290, train_loss_epoch=1.290]        \n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 62.41it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816]\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.816, train_loss_epoch=0.816]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.460, train_loss_epoch=1.460]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.150, train_loss_epoch=1.150]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.857, train_loss_epoch=0.857]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.767, train_loss_epoch=0.767]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.897, train_loss_epoch=0.897]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.350, train_loss_epoch=1.350]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.230, train_loss_epoch=1.230]        \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 31.59it/s, v_num=0, train_loss_step=0.772, train_loss_epoch=0.772]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.772, train_loss_epoch=0.772]        \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 63.92it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200]\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.876, train_loss_epoch=0.876]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070]        \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 63.92it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 36.32it/s, v_num=0, train_loss_step=0.971, train_loss_epoch=0.688]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 30.94it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=12.40]        \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 37.16it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=12.40]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=12.40]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.697, train_loss_epoch=0.697, valid_loss=12.40]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.410, train_loss_epoch=1.410, valid_loss=12.40]        \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.880, train_loss_epoch=0.880, valid_loss=12.40]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.828, train_loss_epoch=0.828, valid_loss=12.40]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.210, train_loss_epoch=1.210, valid_loss=12.40]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.799, train_loss_epoch=0.799, valid_loss=12.40]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=12.40]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.260, train_loss_epoch=1.260, valid_loss=12.40]         \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=12.40]        \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.999, train_loss_epoch=0.999, valid_loss=12.40]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926, valid_loss=12.40]        \n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00, 66.23it/s, v_num=0, train_loss_step=0.772, train_loss_epoch=0.772, valid_loss=12.40]\n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.772, train_loss_epoch=0.772, valid_loss=12.40]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=12.40]        \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=12.40]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=12.40]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=12.40]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.330, train_loss_epoch=1.330, valid_loss=12.40]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=12.40]         \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.804, train_loss_epoch=0.804, valid_loss=12.40]        \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=12.40]        \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.793, train_loss_epoch=0.793, valid_loss=12.40]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.774, train_loss_epoch=0.774, valid_loss=12.40]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.728, train_loss_epoch=0.728, valid_loss=12.40]        \n",
      "Epoch 190: 100%|██████████| 1/1 [00:00<00:00, 49.67it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=0.879, valid_loss=12.40]\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130, valid_loss=12.40]        \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=12.40]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 64.00it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=0.777, valid_loss=12.40] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 21.13it/s]\u001b[A\n",
      "                                                                      \u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.80]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.825, train_loss_epoch=0.825, valid_loss=10.80]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.948, train_loss_epoch=0.948, valid_loss=10.80]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899, valid_loss=10.80]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=10.80]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.320, train_loss_epoch=1.320, valid_loss=10.80]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=10.80]        \n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 44.41it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=10.80]\n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=10.80]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=10.80]        \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.844, train_loss_epoch=0.844, valid_loss=10.80]        \n",
      "Epoch 237: 100%|██████████| 1/1 [00:00<00:00, 47.28it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=10.80]\n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.862, train_loss_epoch=0.862, valid_loss=10.80]        \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960, valid_loss=10.80]        \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.858, train_loss_epoch=0.858, valid_loss=10.80]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=10.80]         \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 62.41it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=1.020, valid_loss=10.80]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803, valid_loss=10.80]        \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.770, train_loss_epoch=0.770, valid_loss=10.80]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.971, train_loss_epoch=0.971, valid_loss=10.80]        \n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00, 58.08it/s, v_num=0, train_loss_step=0.807, train_loss_epoch=0.697, valid_loss=10.80]\n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.807, train_loss_epoch=0.807, valid_loss=10.80]        \n",
      "Epoch 266: 100%|██████████| 1/1 [00:00<00:00, 56.66it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=10.80]\n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=10.80]        \n",
      "Epoch 267: 100%|██████████| 1/1 [00:00<00:00, 33.71it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.907, valid_loss=10.80]\n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=10.80]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.747, train_loss_epoch=0.747, valid_loss=10.80]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.970, train_loss_epoch=0.970, valid_loss=10.80]        \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.280, train_loss_epoch=1.280, valid_loss=10.80]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960, valid_loss=10.80]        \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=10.80]        \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=10.80]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=10.80]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 47.30it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.290, valid_loss=10.80]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 38.21it/s]\u001b[A\n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.900, train_loss_epoch=0.900, valid_loss=14.60]        \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.220, train_loss_epoch=1.220, valid_loss=14.60]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=14.60]        \n",
      "Epoch 313: 100%|██████████| 1/1 [00:00<00:00, 37.59it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=14.60]\n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=14.60]        \n",
      "Epoch 318: 100%|██████████| 1/1 [00:00<00:00, 31.62it/s, v_num=0, train_loss_step=0.886, train_loss_epoch=0.886, valid_loss=14.60]\n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.886, train_loss_epoch=0.886, valid_loss=14.60]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=14.60]        \n",
      "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=14.60]        \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=14.60]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953, valid_loss=14.60]        \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.140, train_loss_epoch=1.140, valid_loss=14.60]        \n",
      "Epoch 344: 100%|██████████| 1/1 [00:00<00:00, 39.27it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887, valid_loss=14.60]\n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887, valid_loss=14.60]        \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.937, train_loss_epoch=0.937, valid_loss=14.60]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=14.60]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.170, train_loss_epoch=1.170, valid_loss=14.60]        \n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=14.60]        \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905, valid_loss=14.60]        \n",
      "Epoch 361: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905, valid_loss=14.60]\n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.727, train_loss_epoch=0.727, valid_loss=14.60]        \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=14.60]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.776, train_loss_epoch=0.776, valid_loss=14.60]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.974, train_loss_epoch=0.974, valid_loss=14.60]        \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.813, train_loss_epoch=0.813, valid_loss=14.60]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.778, train_loss_epoch=0.778, valid_loss=14.60]        \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.792, train_loss_epoch=0.792, valid_loss=14.60]        \n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.160, train_loss_epoch=1.160, valid_loss=14.60]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.937, train_loss_epoch=0.937, valid_loss=14.60]        \n",
      "Epoch 387: 100%|██████████| 1/1 [00:00<00:00, 31.99it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.937, valid_loss=14.60]\n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.996, valid_loss=14.60]        \n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.986, train_loss_epoch=0.986, valid_loss=14.60]        \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=14.60]        \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=14.60]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=14.60]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 31.99it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.130, valid_loss=14.60]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 32.01it/s]\u001b[A\n",
      "Epoch 400: 100%|██████████| 1/1 [00:00<00:00, 31.58it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=1.050, valid_loss=9.550]\n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=0.934, valid_loss=9.550]        \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=9.550]        \n",
      "Epoch 405:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=9.550]        \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=9.550]        \n",
      "Epoch 412: 100%|██████████| 1/1 [00:00<00:00, 60.36it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=9.550]\n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=9.550]        \n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.100, train_loss_epoch=1.100, valid_loss=9.550]        \n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.818, train_loss_epoch=0.818, valid_loss=9.550]        \n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=9.550]        \n",
      "Epoch 427: 100%|██████████| 1/1 [00:00<00:00, 59.85it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.825, valid_loss=9.550]\n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=9.550]        \n",
      "Epoch 433:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=9.550]        \n",
      "Epoch 437: 100%|██████████| 1/1 [00:00<00:00, 43.12it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=9.550]\n",
      "Epoch 438:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=9.550]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=9.550]        \n",
      "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.869, train_loss_epoch=0.869, valid_loss=9.550]        \n",
      "Epoch 447: 100%|██████████| 1/1 [00:00<00:00, 50.11it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.869, valid_loss=9.550]\n",
      "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=9.550]        \n",
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=9.550]        \n",
      "Epoch 453:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791, valid_loss=9.550]        \n",
      "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.785, train_loss_epoch=0.785, valid_loss=9.550]        \n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=9.550]        \n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.805, train_loss_epoch=0.805, valid_loss=9.550]        \n",
      "Epoch 464:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.200, train_loss_epoch=1.200, valid_loss=9.550]        \n",
      "Epoch 465:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.991, train_loss_epoch=0.991, valid_loss=9.550]        \n",
      "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=9.550]        \n",
      "Epoch 474: 100%|██████████| 1/1 [00:00<00:00, 33.38it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810, valid_loss=9.550]\n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.810, train_loss_epoch=0.810, valid_loss=9.550]        \n",
      "Epoch 475: 100%|██████████| 1/1 [00:00<00:00, 76.25it/s, v_num=0, train_loss_step=0.841, train_loss_epoch=0.841, valid_loss=9.550]\n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.841, train_loss_epoch=0.841, valid_loss=9.550]        \n",
      "Epoch 481:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.943, train_loss_epoch=0.943, valid_loss=9.550]        \n",
      "Epoch 486:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.190, train_loss_epoch=1.190, valid_loss=9.550]        \n",
      "Epoch 490: 100%|██████████| 1/1 [00:00<00:00, 37.93it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949, valid_loss=9.550]\n",
      "Epoch 491:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949, valid_loss=9.550]        \n",
      "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=9.550]        \n",
      "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 64.01it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=1.050, valid_loss=9.550]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 31.97it/s]\u001b[A\n",
      "Epoch 500:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.660]        \n",
      "Epoch 505:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.758, train_loss_epoch=0.758, valid_loss=8.660]        \n",
      "Epoch 510:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.080, train_loss_epoch=1.080, valid_loss=8.660]        \n",
      "Epoch 511:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=8.660]        \n",
      "Epoch 516:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.840, train_loss_epoch=0.840, valid_loss=8.660]        \n",
      "Epoch 521:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=8.660]        \n",
      "Epoch 522:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.795, train_loss_epoch=0.795, valid_loss=8.660]        \n",
      "Epoch 527:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.828, train_loss_epoch=0.828, valid_loss=8.660]        \n",
      "Epoch 532:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=8.660]        \n",
      "Epoch 537:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811, valid_loss=8.660]        \n",
      "Epoch 542:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.992, train_loss_epoch=0.992, valid_loss=8.660]         \n",
      "Epoch 543:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.743, train_loss_epoch=0.743, valid_loss=8.660]        \n",
      "Epoch 548:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.764, train_loss_epoch=0.764, valid_loss=8.660]        \n",
      "Epoch 553:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.901, train_loss_epoch=0.901, valid_loss=8.660]        \n",
      "Epoch 558:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.660]        \n",
      "Epoch 558: 100%|██████████| 1/1 [00:00<00:00, 35.03it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.660]\n",
      "Epoch 559:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=8.660]        \n",
      "Epoch 564:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.831, train_loss_epoch=0.831, valid_loss=8.660]        \n",
      "Epoch 564: 100%|██████████| 1/1 [00:00<00:00, 31.57it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.831, valid_loss=8.660]\n",
      "Epoch 565:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803, valid_loss=8.660]        \n",
      "Epoch 570:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808, valid_loss=8.660]        \n",
      "Epoch 571:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=8.660]        \n",
      "Epoch 576:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.660]        \n",
      "Epoch 581:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=8.660]        \n",
      "Epoch 581: 100%|██████████| 1/1 [00:00<00:00, 39.49it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.660]\n",
      "Epoch 582:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.660]        \n",
      "Epoch 587:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=8.660]        \n",
      "Epoch 588:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.840, train_loss_epoch=0.840, valid_loss=8.660]        \n",
      "Epoch 593:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=8.660]        \n",
      "Epoch 594:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.958, train_loss_epoch=0.958, valid_loss=8.660]         \n",
      "Epoch 599:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885, valid_loss=8.660]        \n",
      "Epoch 599: 100%|██████████| 1/1 [00:00<00:00, 62.36it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.885, valid_loss=8.660]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s]\u001b[A\n",
      "Epoch 603:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=8.810]        \n",
      "Epoch 608:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.990, train_loss_epoch=0.990, valid_loss=8.810]        \n",
      "Epoch 613:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=8.810]        \n",
      "Epoch 618:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.800, train_loss_epoch=0.800, valid_loss=8.810]        \n",
      "Epoch 623:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.772, train_loss_epoch=0.772, valid_loss=8.810]        \n",
      "Epoch 624:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=8.810]        \n",
      "Epoch 629:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=8.810]        \n",
      "Epoch 634:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.914, valid_loss=8.810]        \n",
      "Epoch 635:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=8.810]        \n",
      "Epoch 640:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.764, train_loss_epoch=0.764, valid_loss=8.810]         \n",
      "Epoch 645:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.971, train_loss_epoch=0.971, valid_loss=8.810]        \n",
      "Epoch 650:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.779, train_loss_epoch=0.779, valid_loss=8.810]        \n",
      "Epoch 651:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.260, train_loss_epoch=1.260, valid_loss=8.810]        \n",
      "Epoch 656:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.704, train_loss_epoch=0.704, valid_loss=8.810]        \n",
      "Epoch 661:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941, valid_loss=8.810]        \n",
      "Epoch 661:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=8.810]        \n",
      "Epoch 662:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=8.810]\n",
      "Epoch 663:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=8.810]        \n",
      "Epoch 668:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=8.810]        \n",
      "Epoch 673:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=8.810]        \n",
      "Epoch 674:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.971, train_loss_epoch=0.971, valid_loss=8.810]        \n",
      "Epoch 679:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.700, train_loss_epoch=0.700, valid_loss=8.810]        \n",
      "Epoch 684:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.874, train_loss_epoch=0.874, valid_loss=8.810]        \n",
      "Epoch 689:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=8.810]        \n",
      "Epoch 694:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020, valid_loss=8.810]        \n",
      "Epoch 695:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.780, train_loss_epoch=0.780, valid_loss=8.810]        \n",
      "Epoch 699: 100%|██████████| 1/1 [00:00<00:00, 39.92it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.973, valid_loss=8.810]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 30.85it/s]\u001b[A\n",
      "Epoch 703:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.855, train_loss_epoch=0.855, valid_loss=9.680]        \n",
      "Epoch 703: 100%|██████████| 1/1 [00:00<00:00, 31.56it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=9.680]\n",
      "Epoch 704:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=9.680]        \n",
      "Epoch 709:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.824, train_loss_epoch=0.824, valid_loss=9.680]        \n",
      "Epoch 714:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.120, train_loss_epoch=1.120, valid_loss=9.680]        \n",
      "Epoch 715:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.886, train_loss_epoch=0.886, valid_loss=9.680]        \n",
      "Epoch 720:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.801, train_loss_epoch=0.801, valid_loss=9.680]        \n",
      "Epoch 725:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.070, train_loss_epoch=1.070, valid_loss=9.680]        \n",
      "Epoch 730:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.795, train_loss_epoch=0.795, valid_loss=9.680]        \n",
      "Epoch 735:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.716, train_loss_epoch=0.716, valid_loss=9.680]        \n",
      "Epoch 740:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050, valid_loss=9.680]        \n",
      "Epoch 740: 100%|██████████| 1/1 [00:00<00:00, 38.22it/s, v_num=0, train_loss_step=0.675, train_loss_epoch=1.050, valid_loss=9.680]\n",
      "Epoch 741:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.675, train_loss_epoch=0.675, valid_loss=9.680]        \n",
      "Epoch 746:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.760, train_loss_epoch=0.760, valid_loss=9.680]        \n",
      "Epoch 751:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.771, train_loss_epoch=0.771, valid_loss=9.680]        \n",
      "Epoch 752:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.775, train_loss_epoch=0.775, valid_loss=9.680]        \n",
      "Epoch 752: 100%|██████████| 1/1 [00:00<00:00, 139.49it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.775, valid_loss=9.680]\n",
      "Epoch 753:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926, valid_loss=9.680]         \n",
      "Epoch 758:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=9.680]        \n",
      "Epoch 762:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.786, valid_loss=9.680]        \n",
      "Epoch 763:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.838, train_loss_epoch=0.838, valid_loss=9.680]        \n",
      "Epoch 768:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.864, train_loss_epoch=0.864, valid_loss=9.680]        \n",
      "Epoch 773:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.761, train_loss_epoch=0.761, valid_loss=9.680]        \n",
      "Epoch 774:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940, valid_loss=9.680]        \n",
      "Epoch 779:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=9.680]        \n",
      "Epoch 784:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.711, train_loss_epoch=0.711, valid_loss=9.680]        \n",
      "Epoch 789:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847, valid_loss=9.680]        \n",
      "Epoch 790:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.796, train_loss_epoch=0.796, valid_loss=9.680]        \n",
      "Epoch 795:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=9.680]        \n",
      "Epoch 799: 100%|██████████| 1/1 [00:00<00:00, 32.00it/s, v_num=0, train_loss_step=0.709, train_loss_epoch=0.844, valid_loss=9.680]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 31.56it/s]\u001b[A\n",
      "Epoch 803:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.813, train_loss_epoch=0.813, valid_loss=9.070]        \n",
      "Epoch 804:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850, valid_loss=9.070]        \n",
      "Epoch 804: 100%|██████████| 1/1 [00:00<00:00, 39.33it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.850, valid_loss=9.070]\n",
      "Epoch 805:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=9.070]        \n",
      "Epoch 810:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.960, train_loss_epoch=0.960, valid_loss=9.070]        \n",
      "Epoch 811:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.693, train_loss_epoch=0.693, valid_loss=9.070]        \n",
      "Epoch 816:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.802, train_loss_epoch=0.802, valid_loss=9.070]         \n",
      "Epoch 821:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=9.070]        \n",
      "Epoch 822:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=9.070]        \n",
      "Epoch 827:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.702, train_loss_epoch=0.702, valid_loss=9.070]         \n",
      "Epoch 832: 100%|██████████| 1/1 [00:00<00:00, 62.25it/s, v_num=0, train_loss_step=0.780, train_loss_epoch=0.780, valid_loss=9.070]\n",
      "Epoch 833:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.866, train_loss_epoch=0.866, valid_loss=9.070]        \n",
      "Epoch 838:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=9.070]        \n",
      "Epoch 843:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060, valid_loss=9.070]        \n",
      "Epoch 847:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.925, train_loss_epoch=0.925, valid_loss=9.070]        \n",
      "Epoch 848:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.891, valid_loss=9.070]        \n",
      "Epoch 853:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.978, train_loss_epoch=0.978, valid_loss=9.070]        \n",
      "Epoch 858:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=0.934, valid_loss=9.070]        \n",
      "Epoch 863:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=9.070]        \n",
      "Epoch 868:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905, valid_loss=9.070]        \n",
      "Epoch 873:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.851, train_loss_epoch=0.851, valid_loss=9.070]        \n",
      "Epoch 874:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=9.070]        \n",
      "Epoch 878:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.746, train_loss_epoch=0.746, valid_loss=9.070]        \n",
      "Epoch 879:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.836, train_loss_epoch=0.836, valid_loss=9.070]        \n",
      "Epoch 884:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=9.070]        \n",
      "Epoch 889:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.736, train_loss_epoch=0.736, valid_loss=9.070]        \n",
      "Epoch 894:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=9.070]         \n",
      "Epoch 895:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.646, train_loss_epoch=0.646, valid_loss=9.070]        \n",
      "Epoch 899: 100%|██████████| 1/1 [00:00<00:00, 39.41it/s, v_num=0, train_loss_step=0.709, train_loss_epoch=0.874, valid_loss=9.070]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 29.45it/s]\u001b[A\n",
      "Epoch 903:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=8.540]        \n",
      "Epoch 904:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=8.540]        \n",
      "Epoch 909:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=8.540]        \n",
      "Epoch 914:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962, valid_loss=8.540]        \n",
      "Epoch 915:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=8.540]        \n",
      "Epoch 920:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=8.540]        \n",
      "Epoch 925:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.788, train_loss_epoch=0.788, valid_loss=8.540]        \n",
      "Epoch 929:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=8.540]        \n",
      "Epoch 929: 100%|██████████| 1/1 [00:00<00:00, 40.88it/s, v_num=0, train_loss_step=0.833, train_loss_epoch=0.737, valid_loss=8.540]\n",
      "Epoch 930:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.833, train_loss_epoch=0.833, valid_loss=8.540]        \n",
      "Epoch 930: 100%|██████████| 1/1 [00:00<00:00, 40.77it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843, valid_loss=8.540]\n",
      "Epoch 931:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.843, train_loss_epoch=0.843, valid_loss=8.540]        \n",
      "Epoch 936:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=8.540]        \n",
      "Epoch 940:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949, valid_loss=8.540]        \n",
      "Epoch 945:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.853, train_loss_epoch=0.853, valid_loss=8.540]        \n",
      "Epoch 950:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.807, train_loss_epoch=0.807, valid_loss=8.540]        \n",
      "Epoch 954:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=8.540]        \n",
      "Epoch 955:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757, valid_loss=8.540]        \n",
      "Epoch 960:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.688, train_loss_epoch=0.688, valid_loss=8.540]        \n",
      "Epoch 964:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=8.540]        \n",
      "Epoch 965:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994, valid_loss=8.540]        \n",
      "Epoch 970:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.792, train_loss_epoch=0.792, valid_loss=8.540]        \n",
      "Epoch 974:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=8.540]        \n",
      "Epoch 974: 100%|██████████| 1/1 [00:00<00:00, 31.59it/s, v_num=0, train_loss_step=0.748, train_loss_epoch=0.894, valid_loss=8.540]\n",
      "Epoch 975:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.748, train_loss_epoch=0.748, valid_loss=8.540]        \n",
      "Epoch 979: 100%|██████████| 1/1 [00:00<00:00, 35.49it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=8.540]\n",
      "Epoch 980:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.110, train_loss_epoch=1.110, valid_loss=8.540]        \n",
      "Epoch 984:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.761, train_loss_epoch=0.761, valid_loss=8.540]        \n",
      "Epoch 985:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.697, train_loss_epoch=0.697, valid_loss=8.540]        \n",
      "Epoch 989:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010, valid_loss=8.540]        \n",
      "Epoch 990:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.880, train_loss_epoch=0.880, valid_loss=8.540]        \n",
      "Epoch 994: 100%|██████████| 1/1 [00:00<00:00, 31.62it/s, v_num=0, train_loss_step=0.868, train_loss_epoch=0.868, valid_loss=8.540]\n",
      "Epoch 995:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.868, train_loss_epoch=0.868, valid_loss=8.540]        \n",
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 35.00it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.846, valid_loss=8.540]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=7520)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 31.90it/s]\u001b[A\n",
      "Epoch 999: 100%|██████████| 1/1 [00:00<00:00, 12.88it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.891, valid_loss=7.360]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=7520)\u001b[0m `Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m C:\\Users\\35841\\Desktop\\energy_consumption_modeling\\energyEV\\Lib\\site-packages\\ray\\tune\\integration\\pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m Seed set to 15\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m \n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m   | Name            | Type          | Params | Mode \n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 0 | loss            | MAE           | 0      | train\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 1 | padder          | ConstantPad1d | 0      | train\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 2 | scaler          | TemporalNorm  | 0      | train\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 3 | hist_encoder    | LSTM          | 51.4 K | train\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 4 | context_adapter | Linear        | 15.3 K | train\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 5 | mlp_decoder     | MLP           | 897    | train\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m ----------------------------------------------------------\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 67.6 K    Trainable params\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 67.6 K    Total params\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 0.270     Total estimated model params size (MB)\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 11        Modules in train mode\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.060, train_loss_epoch=1.060]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.050, train_loss_epoch=1.050]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.020, train_loss_epoch=1.020]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.010, train_loss_epoch=1.010]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.999, train_loss_epoch=0.999]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.993, train_loss_epoch=0.993]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.992, train_loss_epoch=0.992]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.992, train_loss_epoch=0.992]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.992, train_loss_epoch=0.992]\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.990, train_loss_epoch=0.990]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.989, train_loss_epoch=0.989]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.989, train_loss_epoch=0.989]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.987, train_loss_epoch=0.987]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.987, train_loss_epoch=0.987]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.986, train_loss_epoch=0.986]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983]        \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.984, train_loss_epoch=0.984]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983]        \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.981, train_loss_epoch=0.981]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.981, train_loss_epoch=0.981]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.981, train_loss_epoch=0.981]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.981, train_loss_epoch=0.981]        \n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979]\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.978, train_loss_epoch=0.978]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.978, train_loss_epoch=0.978]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.977, train_loss_epoch=0.977]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.975, train_loss_epoch=0.975]        \n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00,  9.90it/s, v_num=0, train_loss_step=0.975, train_loss_epoch=0.975]\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.975, train_loss_epoch=0.975]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.973, train_loss_epoch=0.973]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.972, train_loss_epoch=0.972]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.970, train_loss_epoch=0.970]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.966, train_loss_epoch=0.966]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.963, train_loss_epoch=0.963]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.958, train_loss_epoch=0.958]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.943, train_loss_epoch=0.943]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.932, train_loss_epoch=0.932]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.964, train_loss_epoch=0.964]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957]        \n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00,  9.28it/s, v_num=0, train_loss_step=0.954, train_loss_epoch=0.954]\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.954, train_loss_epoch=0.954]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.951, train_loss_epoch=0.951]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.946, train_loss_epoch=0.946]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.944, train_loss_epoch=0.944]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.938, train_loss_epoch=0.938]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=0.934]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.933, train_loss_epoch=0.933]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=0.934]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=0.934]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.931, train_loss_epoch=0.931]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.931, train_loss_epoch=0.931]        \n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 10.13it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928]\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.925]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=9.420]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=9.420]        \n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.924, valid_loss=9.420]\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=9.420]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.923, train_loss_epoch=0.923, valid_loss=9.420]        \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=9.420]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=9.420]        \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=9.420]        \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=9.420]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=9.420]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=0.934, valid_loss=9.420]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=9.420]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.959, train_loss_epoch=0.959, valid_loss=9.420]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=9.420]        \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949, valid_loss=9.420]        \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.953, train_loss_epoch=0.953, valid_loss=9.420]        \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955, valid_loss=9.420]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950, valid_loss=9.420]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.943, train_loss_epoch=0.943, valid_loss=9.420]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.938, train_loss_epoch=0.938, valid_loss=9.420]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.937, train_loss_epoch=0.937, valid_loss=9.420]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935, valid_loss=9.420]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.931, train_loss_epoch=0.931, valid_loss=9.420]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929, valid_loss=9.420]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929, valid_loss=9.420]        \n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929, valid_loss=9.420]\n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.929, train_loss_epoch=0.929, valid_loss=9.420]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927, valid_loss=9.420]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.923, train_loss_epoch=0.923, valid_loss=9.420]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=9.420]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=9.420]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=9.420]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=9.420]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=9.420]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=9.420]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=9.420]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.914, valid_loss=9.420]        \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=9.420]        \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=9.420]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=9.420]        \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911, valid_loss=9.420]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=9.420]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909, valid_loss=9.420]        \n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00, 10.40it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909, valid_loss=9.420]\n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=9.420]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=9.420]        \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=9.420]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=9.420]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.900, train_loss_epoch=0.900, valid_loss=9.420]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.898, train_loss_epoch=0.898, valid_loss=9.420]        \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=9.420]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899, valid_loss=9.420]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=9.420]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888, valid_loss=9.420]        \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.886, train_loss_epoch=0.886, valid_loss=9.420]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.956, train_loss_epoch=0.956, valid_loss=9.420]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=9.420]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926, valid_loss=9.420]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=9.420]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=9.420]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962, valid_loss=9.420]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926, valid_loss=9.420]        \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=9.420]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=9.420]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.980, train_loss_epoch=0.980, valid_loss=9.420]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.982, train_loss_epoch=0.982, valid_loss=9.420]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.980, train_loss_epoch=0.980, valid_loss=9.420]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.970, train_loss_epoch=0.970, valid_loss=9.420]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040, valid_loss=9.420]        \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030, valid_loss=9.420]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=9.420]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.986, train_loss_epoch=0.986, valid_loss=9.420]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.000, train_loss_epoch=1.000, valid_loss=9.420]        \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983, valid_loss=9.420]\n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.983, train_loss_epoch=0.983, valid_loss=9.420]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.984, train_loss_epoch=0.984, valid_loss=9.420]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985, valid_loss=9.420]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.991, train_loss_epoch=0.991, valid_loss=9.420]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997, valid_loss=9.420]        \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.996, valid_loss=9.420]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.994, train_loss_epoch=0.994, valid_loss=9.420]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.991, train_loss_epoch=0.991, valid_loss=9.420]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.988, train_loss_epoch=0.988, valid_loss=9.420]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.980, train_loss_epoch=0.980, valid_loss=9.420]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.977, train_loss_epoch=0.977, valid_loss=9.420]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.973, train_loss_epoch=0.973, valid_loss=9.420]        \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=9.420]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=9.420]\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962, valid_loss=9.420]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.959, train_loss_epoch=0.959, valid_loss=9.420]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.956, train_loss_epoch=0.956, valid_loss=9.420]        \n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s, v_num=0, train_loss_step=0.951, train_loss_epoch=0.952, valid_loss=9.420]\n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.951, train_loss_epoch=0.951, valid_loss=9.420]        \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949, valid_loss=9.420]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.946, train_loss_epoch=0.946, valid_loss=9.420]        \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942, valid_loss=9.420]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.942, valid_loss=9.420]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 31.58it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939, valid_loss=12.30]        \n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=12.30]        \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.934, train_loss_epoch=0.934, valid_loss=12.30]        \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=12.30]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927, valid_loss=12.30]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.996, train_loss_epoch=0.996, valid_loss=12.30]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.991, train_loss_epoch=0.991, valid_loss=12.30]        \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.988, train_loss_epoch=0.988, valid_loss=12.30]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.985, train_loss_epoch=0.985, valid_loss=12.30]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.981, train_loss_epoch=0.981, valid_loss=12.30]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=12.30]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=12.30]        \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.979, train_loss_epoch=0.979, valid_loss=12.30]        \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.977, train_loss_epoch=0.977, valid_loss=12.30]        \n",
      "Epoch 214: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s, v_num=0, train_loss_step=0.976, train_loss_epoch=0.976, valid_loss=12.30]\n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.976, train_loss_epoch=0.976, valid_loss=12.30]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.974, train_loss_epoch=0.974, valid_loss=12.30]        \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.972, train_loss_epoch=0.972, valid_loss=12.30]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.971, train_loss_epoch=0.971, valid_loss=12.30]        \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 10.01it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=12.30]\n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.969, train_loss_epoch=0.969, valid_loss=12.30]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.968, train_loss_epoch=0.968, valid_loss=12.30]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.967, train_loss_epoch=0.967, valid_loss=12.30]        \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.967, train_loss_epoch=0.967, valid_loss=12.30]        \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.965, train_loss_epoch=0.965, valid_loss=12.30]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.964, train_loss_epoch=0.964, valid_loss=12.30]        \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.962, train_loss_epoch=0.962, valid_loss=12.30]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.961, train_loss_epoch=0.961, valid_loss=12.30]        \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.959, train_loss_epoch=0.959, valid_loss=12.30]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=12.30]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955, valid_loss=12.30]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.954, train_loss_epoch=0.954, valid_loss=12.30]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=12.30]        \n",
      "Epoch 231: 100%|██████████| 1/1 [00:00<00:00,  9.16it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=12.30]\n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.950, train_loss_epoch=0.950, valid_loss=12.30]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.948, train_loss_epoch=0.948, valid_loss=12.30]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=12.30]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.944, train_loss_epoch=0.944, valid_loss=12.30]        \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940, valid_loss=12.30]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939, valid_loss=12.30]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935, valid_loss=12.30]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.931, train_loss_epoch=0.931, valid_loss=12.30]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=12.30]        \n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926, valid_loss=12.30]        \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=12.30]        \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=12.30]        \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=12.30]        \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941, valid_loss=12.30]        \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=12.30]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941, valid_loss=12.30]        \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=12.30]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.915, train_loss_epoch=0.915, valid_loss=12.30]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=12.30]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=12.30]        \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927, valid_loss=12.30]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=12.30]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909, valid_loss=12.30]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=12.30]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=12.30]        \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=12.30]        \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.942, train_loss_epoch=0.942, valid_loss=12.30]        \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.926, train_loss_epoch=0.926, valid_loss=12.30]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.995, train_loss_epoch=0.995, valid_loss=12.30]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997, valid_loss=12.30]        \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.987, train_loss_epoch=0.987, valid_loss=12.30]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.952, train_loss_epoch=0.952, valid_loss=12.30]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939, valid_loss=12.30]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941, valid_loss=12.30]        \n",
      "Epoch 271: 100%|██████████| 1/1 [00:00<00:00,  8.95it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.951, valid_loss=12.30]\n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.957, train_loss_epoch=0.957, valid_loss=12.30]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.947, train_loss_epoch=0.947, valid_loss=12.30]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941, valid_loss=12.30]        \n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.939, train_loss_epoch=0.939, valid_loss=12.30]        \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=12.30]        \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.925, train_loss_epoch=0.925, valid_loss=12.30]        \n",
      "Epoch 278: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.917, valid_loss=12.30]\n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.914, train_loss_epoch=0.914, valid_loss=12.30]        \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=12.30]        \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.917, train_loss_epoch=0.917, valid_loss=12.30]        \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.911, train_loss_epoch=0.911, valid_loss=12.30]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.935, train_loss_epoch=0.935, valid_loss=12.30]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.930, train_loss_epoch=0.930, valid_loss=12.30]        \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.928, train_loss_epoch=0.928, valid_loss=12.30]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.921, train_loss_epoch=0.921, valid_loss=12.30]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.919, train_loss_epoch=0.919, valid_loss=12.30]        \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.913, train_loss_epoch=0.913, valid_loss=12.30]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=12.30]        \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899, valid_loss=12.30]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=12.30]        \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=12.30]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=12.30]        \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=12.30]        \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=12.30]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=12.30]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 11.29it/s, v_num=0, train_loss_step=0.908, train_loss_epoch=0.910, valid_loss=12.30]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(_train_tune pid=36116)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.908, train_loss_epoch=0.908, valid_loss=7.390]        \n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=7.390]        \n",
      "Epoch 301: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=7.390]\n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=7.390]        \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905, valid_loss=7.390]        \n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=7.390]        \n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=7.390]        \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887, valid_loss=7.390]        \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881, valid_loss=7.390]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=7.390]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=7.390]        \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=7.390]        \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=7.390]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=7.390]        \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=7.390]        \n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.882, train_loss_epoch=0.882, valid_loss=7.390]        \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=7.390]        \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=7.390]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=7.390]        \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.910, train_loss_epoch=0.910, valid_loss=7.390]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.925, train_loss_epoch=0.925, valid_loss=7.390]        \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.904, train_loss_epoch=0.904, valid_loss=7.390]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905, valid_loss=7.390]        \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=7.390]        \n",
      "Epoch 326: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s, v_num=0, train_loss_step=0.900, train_loss_epoch=0.900, valid_loss=7.390]\n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.900, train_loss_epoch=0.900, valid_loss=7.390]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.894, train_loss_epoch=0.894, valid_loss=7.390]        \n",
      "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.886, train_loss_epoch=0.886, valid_loss=7.390]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=7.390]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=7.390]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.891, train_loss_epoch=0.891, valid_loss=7.390]        \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.890, train_loss_epoch=0.890, valid_loss=7.390]        \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.889, train_loss_epoch=0.889, valid_loss=7.390]        \n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=7.390]        \n",
      "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.877, train_loss_epoch=0.877, valid_loss=7.390]        \n",
      "Epoch 338:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=7.390]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872, valid_loss=7.390]        \n",
      "Epoch 340: 100%|██████████| 1/1 [00:00<00:00,  9.65it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.870, valid_loss=7.390]\n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.873, train_loss_epoch=0.873, valid_loss=7.390]        \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.932, train_loss_epoch=0.932, valid_loss=7.390]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.890, train_loss_epoch=0.890, valid_loss=7.390]        \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=7.390]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=7.390]        \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=7.390]        \n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.956, train_loss_epoch=0.956, valid_loss=7.390]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.922, train_loss_epoch=0.922, valid_loss=7.390]        \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.907, train_loss_epoch=0.907, valid_loss=7.390]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=7.390]        \n",
      "Epoch 351: 100%|██████████| 1/1 [00:00<00:00,  9.94it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.920, valid_loss=7.390]\n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.924, train_loss_epoch=0.924, valid_loss=7.390]        \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.897, train_loss_epoch=0.897, valid_loss=7.390]        \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=7.390]        \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.901, train_loss_epoch=0.901, valid_loss=7.390]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.902, train_loss_epoch=0.902, valid_loss=7.390]        \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.906, train_loss_epoch=0.906, valid_loss=7.390]        \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.943, train_loss_epoch=0.943, valid_loss=7.390]        \n",
      "Epoch 359:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.972, train_loss_epoch=0.972, valid_loss=7.390]        \n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.955, train_loss_epoch=0.955, valid_loss=7.390]        \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949, valid_loss=7.390]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.949, train_loss_epoch=0.949, valid_loss=7.390]        \n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.943, train_loss_epoch=0.943, valid_loss=7.390]        \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.937, train_loss_epoch=0.937, valid_loss=7.390]        \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.936, train_loss_epoch=0.936, valid_loss=7.390]        \n",
      "Epoch 366:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.938, train_loss_epoch=0.938, valid_loss=7.390]        \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.941, train_loss_epoch=0.941, valid_loss=7.390]        \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.940, train_loss_epoch=0.940, valid_loss=7.390]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.927, train_loss_epoch=0.927, valid_loss=7.390]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.920, train_loss_epoch=0.920, valid_loss=7.390]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.916, train_loss_epoch=0.916, valid_loss=7.390]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912, valid_loss=7.390]        \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899, valid_loss=7.390]        \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=7.390]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.895, train_loss_epoch=0.895, valid_loss=7.390]        \n",
      "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=7.390]        \n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.887, train_loss_epoch=0.887, valid_loss=7.390]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881, valid_loss=7.390]        \n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=7.390]        \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.878, train_loss_epoch=0.878, valid_loss=7.390]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.882, train_loss_epoch=0.882, valid_loss=7.390]        \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.881, train_loss_epoch=0.881, valid_loss=7.390]        \n",
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.875, train_loss_epoch=0.875, valid_loss=7.390]        \n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.868, train_loss_epoch=0.868, valid_loss=7.390]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.892, train_loss_epoch=0.892, valid_loss=7.390]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.884, train_loss_epoch=0.884, valid_loss=7.390]        \n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.875, train_loss_epoch=0.875, valid_loss=7.390]        \n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=7.390]        \n",
      "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.903, train_loss_epoch=0.903, valid_loss=7.390]        \n",
      "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.879, train_loss_epoch=0.879, valid_loss=7.390]        \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.880, train_loss_epoch=0.880, valid_loss=7.390]        \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.883, train_loss_epoch=0.883, valid_loss=7.390]        \n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.882, train_loss_epoch=0.882, valid_loss=7.390]        \n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.870, train_loss_epoch=0.870, valid_loss=7.390]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.867, train_loss_epoch=0.867, valid_loss=7.390]        \n",
      "Epoch 399:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.866, train_loss_epoch=0.866, valid_loss=7.390]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s, v_num=0, train_loss_step=0.865, train_loss_epoch=0.866, valid_loss=7.390]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 25.45it/s]\u001b[A\n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.865, train_loss_epoch=0.865, valid_loss=8.700]        \n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.863, train_loss_epoch=0.863, valid_loss=8.700]        \n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.858, train_loss_epoch=0.858, valid_loss=8.700]        \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850, valid_loss=8.700]        \n",
      "Epoch 405:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850, valid_loss=8.700]        \n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.855, train_loss_epoch=0.855, valid_loss=8.700]        \n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.844, train_loss_epoch=0.844, valid_loss=8.700]        \n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.856, train_loss_epoch=0.856, valid_loss=8.700]        \n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.852, train_loss_epoch=0.852, valid_loss=8.700]        \n",
      "Epoch 411:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.846, train_loss_epoch=0.846, valid_loss=8.700]        \n",
      "Epoch 412:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.700]        \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847, valid_loss=8.700]        \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847, valid_loss=8.700]        \n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.845, train_loss_epoch=0.845, valid_loss=8.700]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.842, train_loss_epoch=0.842, valid_loss=8.700]        \n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839, valid_loss=8.700]        \n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.836, train_loss_epoch=0.836, valid_loss=8.700]        \n",
      "Epoch 420:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.835, train_loss_epoch=0.835, valid_loss=8.700]        \n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.832, train_loss_epoch=0.832, valid_loss=8.700]        \n",
      "Epoch 422:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.830, train_loss_epoch=0.830, valid_loss=8.700]        \n",
      "Epoch 423: 100%|██████████| 1/1 [00:00<00:00, 10.18it/s, v_num=0, train_loss_step=0.823, train_loss_epoch=0.823, valid_loss=8.700]\n",
      "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.823, train_loss_epoch=0.823, valid_loss=8.700]        \n",
      "Epoch 425:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.823, train_loss_epoch=0.823, valid_loss=8.700]        \n",
      "Epoch 426:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.821, train_loss_epoch=0.821, valid_loss=8.700]        \n",
      "Epoch 427:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.822, train_loss_epoch=0.822, valid_loss=8.700]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.817, train_loss_epoch=0.817, valid_loss=8.700]        \n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.817, train_loss_epoch=0.817, valid_loss=8.700]        \n",
      "Epoch 431:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.814, train_loss_epoch=0.814, valid_loss=8.700]        \n",
      "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.812, train_loss_epoch=0.812, valid_loss=8.700]        \n",
      "Epoch 433:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.811, train_loss_epoch=0.811, valid_loss=8.700]        \n",
      "Epoch 433: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809, valid_loss=8.700]\n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809, valid_loss=8.700]        \n",
      "Epoch 435:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806, valid_loss=8.700]        \n",
      "Epoch 436:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.805, train_loss_epoch=0.805, valid_loss=8.700]        \n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.803, train_loss_epoch=0.803, valid_loss=8.700]        \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.801, train_loss_epoch=0.801, valid_loss=8.700]        \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798, valid_loss=8.700]        \n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.795, train_loss_epoch=0.795, valid_loss=8.700]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.794, train_loss_epoch=0.794, valid_loss=8.700]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.792, train_loss_epoch=0.792, valid_loss=8.700]        \n",
      "Epoch 444:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.791, train_loss_epoch=0.791, valid_loss=8.700]        \n",
      "Epoch 445:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.789, train_loss_epoch=0.789, valid_loss=8.700]        \n",
      "Epoch 446:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.787, train_loss_epoch=0.787, valid_loss=8.700]        \n",
      "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.784, train_loss_epoch=0.784, valid_loss=8.700]        \n",
      "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.782, train_loss_epoch=0.782, valid_loss=8.700]        \n",
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.700]        \n",
      "Epoch 450:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.780, train_loss_epoch=0.780, valid_loss=8.700]        \n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.778, train_loss_epoch=0.778, valid_loss=8.700]        \n",
      "Epoch 452:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.775, train_loss_epoch=0.775, valid_loss=8.700]        \n",
      "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.773, train_loss_epoch=0.773, valid_loss=8.700]        \n",
      "Epoch 455:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.770, train_loss_epoch=0.770, valid_loss=8.700]        \n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.768, train_loss_epoch=0.768, valid_loss=8.700]        \n",
      "Epoch 457:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.766, train_loss_epoch=0.766, valid_loss=8.700]        \n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.765, train_loss_epoch=0.765, valid_loss=8.700]        \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.769, train_loss_epoch=0.769, valid_loss=8.700]        \n",
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.764, train_loss_epoch=0.764, valid_loss=8.700]        \n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.785, train_loss_epoch=0.785, valid_loss=8.700]        \n",
      "Epoch 462:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.820, train_loss_epoch=0.820, valid_loss=8.700]        \n",
      "Epoch 463:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.830, train_loss_epoch=0.830, valid_loss=8.700]        \n",
      "Epoch 464:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.797, train_loss_epoch=0.797, valid_loss=8.700]        \n",
      "Epoch 465:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834, valid_loss=8.700]        \n",
      "Epoch 466:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.827, train_loss_epoch=0.827, valid_loss=8.700]        \n",
      "Epoch 467:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.807, train_loss_epoch=0.807, valid_loss=8.700]        \n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.805, train_loss_epoch=0.805, valid_loss=8.700]        \n",
      "Epoch 469: 100%|██████████| 1/1 [00:00<00:00, 11.81it/s, v_num=0, train_loss_step=0.805, train_loss_epoch=0.805, valid_loss=8.700]\n",
      "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.805, train_loss_epoch=0.805, valid_loss=8.700]        \n",
      "Epoch 471:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.797, train_loss_epoch=0.797, valid_loss=8.700]        \n",
      "Epoch 472:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.796, train_loss_epoch=0.796, valid_loss=8.700]        \n",
      "Epoch 473:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.795, train_loss_epoch=0.795, valid_loss=8.700]        \n",
      "Epoch 474: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s, v_num=0, train_loss_step=0.793, train_loss_epoch=0.793, valid_loss=8.700]\n",
      "Epoch 474: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.793, valid_loss=8.700]\n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.786, valid_loss=8.700]        \n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.700]        \n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.779, train_loss_epoch=0.779, valid_loss=8.700]        \n",
      "Epoch 478:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.781, train_loss_epoch=0.781, valid_loss=8.700]        \n",
      "Epoch 479:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.779, train_loss_epoch=0.779, valid_loss=8.700]        \n",
      "Epoch 481:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.768, train_loss_epoch=0.768, valid_loss=8.700]        \n",
      "Epoch 482:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.765, train_loss_epoch=0.765, valid_loss=8.700]        \n",
      "Epoch 483:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.766, train_loss_epoch=0.766, valid_loss=8.700]        \n",
      "Epoch 484:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.762, train_loss_epoch=0.762, valid_loss=8.700]        \n",
      "Epoch 485:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.759, train_loss_epoch=0.759, valid_loss=8.700]        \n",
      "Epoch 486:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.758, train_loss_epoch=0.758, valid_loss=8.700]        \n",
      "Epoch 487:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.758, train_loss_epoch=0.758, valid_loss=8.700]        \n",
      "Epoch 488:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.755, train_loss_epoch=0.755, valid_loss=8.700]        \n",
      "Epoch 489:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.753, train_loss_epoch=0.753, valid_loss=8.700]        \n",
      "Epoch 490:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.752, train_loss_epoch=0.752, valid_loss=8.700]        \n",
      "Epoch 491:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.751, train_loss_epoch=0.751, valid_loss=8.700]        \n",
      "Epoch 492:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.749, train_loss_epoch=0.749, valid_loss=8.700]        \n",
      "Epoch 493:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.746, train_loss_epoch=0.746, valid_loss=8.700]        \n",
      "Epoch 494:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.745, train_loss_epoch=0.745, valid_loss=8.700]        \n",
      "Epoch 495:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.744, train_loss_epoch=0.744, valid_loss=8.700]        \n",
      "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.741, train_loss_epoch=0.741, valid_loss=8.700]        \n",
      "Epoch 497: 100%|██████████| 1/1 [00:00<00:00,  9.05it/s, v_num=0, train_loss_step=0.739, train_loss_epoch=0.739, valid_loss=8.700]\n",
      "Epoch 498:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.739, train_loss_epoch=0.739, valid_loss=8.700]        \n",
      "Epoch 499:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=8.700]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_train_tune pid=36116)\u001b[0m `Trainer.fit` stopped: `max_steps=500` reached.\n",
      "2024-12-05 18:38:25,070\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/35841/ray_results/_train_tune_2024-12-05_18-24-01' in 0.0203s.\n",
      "Seed set to 10\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type          | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | loss            | MAE           | 0      | eval \n",
      "1 | padder          | ConstantPad1d | 0      | train\n",
      "2 | scaler          | TemporalNorm  | 0      | train\n",
      "3 | hist_encoder    | LSTM          | 31.0 K | train\n",
      "4 | context_adapter | Linear        | 153 K  | train\n",
      "5 | mlp_decoder     | MLP           | 3.3 K  | train\n",
      "----------------------------------------------------------\n",
      "187 K     Trainable params\n",
      "0         Non-trainable params\n",
      "187 K     Total params\n",
      "0.749     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=8.700]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 16.80it/s]\u001b[A\n",
      "Epoch 499: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s, v_num=0, train_loss_step=0.737, train_loss_epoch=0.737, valid_loss=9.350]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aeb2c4b8c2c419daa8c67d6a9b4fd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b529923048d24414a8a960fe5a342f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbf163c45ee49378e195b1926148431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f0aae9e324429d9641f3ddeb7e030c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c7ff9ce0cd47fb973518b410b6531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7ab26716c14e42840dedccc7c3b817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e173e7edf004237a373cdfcbc5ce694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94b748a16a04e2abf20f92fe996e0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86635735ed4f4eca83473af2eb5276ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a568310410546dfb3ef115c7aecc77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6896061ff0794267b6ed96795bdab7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ce6b2ee7ec45d98d8821b418d4360d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    }
   ],
   "source": [
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        AutoNHITS(h=2*30, num_samples=5),\n",
    "        AutoLSTM(h=2*30, num_samples=5),\n",
    "    ],\n",
    "    freq='D'\n",
    ")\n",
    "nf.fit(df=Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e539e209-c31f-4282-8c9a-c13773e30b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a280a47f516042f2997a8bb2569326ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7c6b33a1574a429fc2fdc59f8de0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\35841\\Desktop\\energy_consumption_modeling\\energyEV\\Lib\\site-packages\\neuralforecast\\core.py:214: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fcst_df = nf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7ae640-9e24-4a80-93f5-d14066a3b48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>AutoNHITS</th>\n",
       "      <th>AutoLSTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>44.723534</td>\n",
       "      <td>12.581085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>48.808212</td>\n",
       "      <td>25.406292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-03</td>\n",
       "      <td>39.117203</td>\n",
       "      <td>39.343071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-04</td>\n",
       "      <td>29.631641</td>\n",
       "      <td>40.147255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-05</td>\n",
       "      <td>23.188076</td>\n",
       "      <td>39.718605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>28.528814</td>\n",
       "      <td>42.766407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-07</td>\n",
       "      <td>26.518375</td>\n",
       "      <td>37.581017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-08</td>\n",
       "      <td>25.463428</td>\n",
       "      <td>13.328389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-09</td>\n",
       "      <td>27.549442</td>\n",
       "      <td>23.829197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-10</td>\n",
       "      <td>24.730082</td>\n",
       "      <td>40.037437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-11</td>\n",
       "      <td>27.160503</td>\n",
       "      <td>41.718895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-12</td>\n",
       "      <td>22.752834</td>\n",
       "      <td>39.550797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-13</td>\n",
       "      <td>42.611053</td>\n",
       "      <td>41.219742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-14</td>\n",
       "      <td>25.752424</td>\n",
       "      <td>34.637066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-15</td>\n",
       "      <td>27.411211</td>\n",
       "      <td>10.909587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>29.359718</td>\n",
       "      <td>23.937826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>24.390373</td>\n",
       "      <td>37.812576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>26.572763</td>\n",
       "      <td>39.434135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>25.341314</td>\n",
       "      <td>36.763222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-20</td>\n",
       "      <td>25.432007</td>\n",
       "      <td>40.811119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-21</td>\n",
       "      <td>23.039585</td>\n",
       "      <td>35.790798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-22</td>\n",
       "      <td>24.392719</td>\n",
       "      <td>9.628778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-23</td>\n",
       "      <td>34.298210</td>\n",
       "      <td>20.866360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-24</td>\n",
       "      <td>39.379871</td>\n",
       "      <td>35.747448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-25</td>\n",
       "      <td>26.147409</td>\n",
       "      <td>38.612480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>27.407242</td>\n",
       "      <td>37.518101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-27</td>\n",
       "      <td>20.601126</td>\n",
       "      <td>37.558746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-28</td>\n",
       "      <td>22.402395</td>\n",
       "      <td>36.049583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-29</td>\n",
       "      <td>23.657709</td>\n",
       "      <td>9.526038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>26.771275</td>\n",
       "      <td>21.271957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>24.607464</td>\n",
       "      <td>36.085442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-02</td>\n",
       "      <td>24.652275</td>\n",
       "      <td>38.804291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-03</td>\n",
       "      <td>27.931032</td>\n",
       "      <td>37.831070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-04</td>\n",
       "      <td>21.951536</td>\n",
       "      <td>37.421295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-05</td>\n",
       "      <td>22.135206</td>\n",
       "      <td>33.503551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-06</td>\n",
       "      <td>25.511478</td>\n",
       "      <td>5.551287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>26.833075</td>\n",
       "      <td>17.379858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-08</td>\n",
       "      <td>24.938456</td>\n",
       "      <td>33.969467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-09</td>\n",
       "      <td>26.577030</td>\n",
       "      <td>35.970184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-10</td>\n",
       "      <td>28.555508</td>\n",
       "      <td>36.253696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-11</td>\n",
       "      <td>24.268063</td>\n",
       "      <td>37.569595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-12</td>\n",
       "      <td>23.205795</td>\n",
       "      <td>31.947668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-13</td>\n",
       "      <td>26.532223</td>\n",
       "      <td>9.181620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>28.117283</td>\n",
       "      <td>18.232830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>27.906185</td>\n",
       "      <td>32.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-16</td>\n",
       "      <td>24.762266</td>\n",
       "      <td>37.936527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-17</td>\n",
       "      <td>27.497875</td>\n",
       "      <td>36.200912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-18</td>\n",
       "      <td>32.704140</td>\n",
       "      <td>39.931969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-19</td>\n",
       "      <td>23.437244</td>\n",
       "      <td>36.049831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-20</td>\n",
       "      <td>24.856285</td>\n",
       "      <td>11.045477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-21</td>\n",
       "      <td>30.969448</td>\n",
       "      <td>21.020893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-22</td>\n",
       "      <td>30.001942</td>\n",
       "      <td>35.807854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-23</td>\n",
       "      <td>23.914570</td>\n",
       "      <td>38.685066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-24</td>\n",
       "      <td>29.572031</td>\n",
       "      <td>37.609131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-25</td>\n",
       "      <td>24.101173</td>\n",
       "      <td>41.226791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-26</td>\n",
       "      <td>24.333950</td>\n",
       "      <td>38.147491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>25.912479</td>\n",
       "      <td>11.470265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-28</td>\n",
       "      <td>25.502226</td>\n",
       "      <td>21.604445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-29</td>\n",
       "      <td>24.250225</td>\n",
       "      <td>34.707260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>24.905331</td>\n",
       "      <td>39.732655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds  AutoNHITS   AutoLSTM\n",
       "unique_id                                 \n",
       "1         2024-09-01  44.723534  12.581085\n",
       "1         2024-09-02  48.808212  25.406292\n",
       "1         2024-09-03  39.117203  39.343071\n",
       "1         2024-09-04  29.631641  40.147255\n",
       "1         2024-09-05  23.188076  39.718605\n",
       "1         2024-09-06  28.528814  42.766407\n",
       "1         2024-09-07  26.518375  37.581017\n",
       "1         2024-09-08  25.463428  13.328389\n",
       "1         2024-09-09  27.549442  23.829197\n",
       "1         2024-09-10  24.730082  40.037437\n",
       "1         2024-09-11  27.160503  41.718895\n",
       "1         2024-09-12  22.752834  39.550797\n",
       "1         2024-09-13  42.611053  41.219742\n",
       "1         2024-09-14  25.752424  34.637066\n",
       "1         2024-09-15  27.411211  10.909587\n",
       "1         2024-09-16  29.359718  23.937826\n",
       "1         2024-09-17  24.390373  37.812576\n",
       "1         2024-09-18  26.572763  39.434135\n",
       "1         2024-09-19  25.341314  36.763222\n",
       "1         2024-09-20  25.432007  40.811119\n",
       "1         2024-09-21  23.039585  35.790798\n",
       "1         2024-09-22  24.392719   9.628778\n",
       "1         2024-09-23  34.298210  20.866360\n",
       "1         2024-09-24  39.379871  35.747448\n",
       "1         2024-09-25  26.147409  38.612480\n",
       "1         2024-09-26  27.407242  37.518101\n",
       "1         2024-09-27  20.601126  37.558746\n",
       "1         2024-09-28  22.402395  36.049583\n",
       "1         2024-09-29  23.657709   9.526038\n",
       "1         2024-09-30  26.771275  21.271957\n",
       "1         2024-10-01  24.607464  36.085442\n",
       "1         2024-10-02  24.652275  38.804291\n",
       "1         2024-10-03  27.931032  37.831070\n",
       "1         2024-10-04  21.951536  37.421295\n",
       "1         2024-10-05  22.135206  33.503551\n",
       "1         2024-10-06  25.511478   5.551287\n",
       "1         2024-10-07  26.833075  17.379858\n",
       "1         2024-10-08  24.938456  33.969467\n",
       "1         2024-10-09  26.577030  35.970184\n",
       "1         2024-10-10  28.555508  36.253696\n",
       "1         2024-10-11  24.268063  37.569595\n",
       "1         2024-10-12  23.205795  31.947668\n",
       "1         2024-10-13  26.532223   9.181620\n",
       "1         2024-10-14  28.117283  18.232830\n",
       "1         2024-10-15  27.906185  32.636364\n",
       "1         2024-10-16  24.762266  37.936527\n",
       "1         2024-10-17  27.497875  36.200912\n",
       "1         2024-10-18  32.704140  39.931969\n",
       "1         2024-10-19  23.437244  36.049831\n",
       "1         2024-10-20  24.856285  11.045477\n",
       "1         2024-10-21  30.969448  21.020893\n",
       "1         2024-10-22  30.001942  35.807854\n",
       "1         2024-10-23  23.914570  38.685066\n",
       "1         2024-10-24  29.572031  37.609131\n",
       "1         2024-10-25  24.101173  41.226791\n",
       "1         2024-10-26  24.333950  38.147491\n",
       "1         2024-10-27  25.912479  11.470265\n",
       "1         2024-10-28  25.502226  21.604445\n",
       "1         2024-10-29  24.250225  34.707260\n",
       "1         2024-10-30  24.905331  39.732655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e8cc04e-ff30-4c3b-bf02-aad6b71eb18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoNHITS\n",
      "AutoLSTM\n"
     ]
    }
   ],
   "source": [
    "for model in nf.models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21b92ccc-fbc1-4837-bfb9-31ebf32e992c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>7.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>6.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>54.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>27.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>2024-08-28</td>\n",
       "      <td>28.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>32.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>2024-08-30</td>\n",
       "      <td>32.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>29.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1705 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds      y  unique_id\n",
       "0    2020-01-01   1.49          1\n",
       "1    2020-01-02   7.47          1\n",
       "2    2020-01-03   6.05          1\n",
       "3    2020-01-04   6.53          1\n",
       "4    2020-01-05  54.61          1\n",
       "...         ...    ...        ...\n",
       "1700 2024-08-27  27.76          1\n",
       "1701 2024-08-28  28.32          1\n",
       "1702 2024-08-29  32.08          1\n",
       "1703 2024-08-30  32.67          1\n",
       "1704 2024-08-31  29.56          1\n",
       "\n",
       "[1705 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9f35c0b-2a5c-4adf-acfa-ee431386c26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unique_id         ds  AutoNHITS   AutoLSTM\n",
      "0          1 2024-09-01  44.723534  12.581085\n",
      "1          1 2024-09-02  48.808212  25.406292\n",
      "2          1 2024-09-03  39.117203  39.343071\n",
      "3          1 2024-09-04  29.631641  40.147255\n",
      "4          1 2024-09-05  23.188076  39.718605\n"
     ]
    }
   ],
   "source": [
    "# Reset index of fcst_df to ensure 'unique_id' is a column\n",
    "fcst_df = fcst_df.reset_index()\n",
    "\n",
    "# Print the first few rows of fcst_df to check\n",
    "print(fcst_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ef0a1f8-7b2d-4859-a0be-9b27c8284d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = check_df.rename(columns={'actual': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3980d5f-7868-470a-8fcc-1467b0a9a425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x350 with 1 Axes>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsforecast import StatsForecast\n",
    "StatsForecast.plot(check_df, fcst_df, engine='matplotlib', max_insample_length=30 * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "945872a2-c178-4dfa-887d-83dda1ffe25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoNHITS"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7fd5e5e-3b7d-485b-8f60-9c7d1be82311",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.save(path='./autonhits_model/',\n",
    "        model_index=[0], \n",
    "        overwrite=True,\n",
    "        save_dataset=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e5dc8-eec3-4e91-9382-20bd799f85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nf2 = NeuralForecast.load(path='./autonhits_model/')\n",
    "#Y_hat_df = nf2.predict().reset_index()\n",
    "#Y_hat_df.head()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnergyEV",
   "language": "python",
   "name": "energyev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
